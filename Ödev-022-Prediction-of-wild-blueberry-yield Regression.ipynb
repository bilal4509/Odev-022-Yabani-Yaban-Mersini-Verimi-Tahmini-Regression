{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='11.jpg'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Yaban mersini zengin antioksidan içeriği ile kolesterol seviyesini kontrol etmeye de yardımcı olmaktadır. Çalışmalar, bu bileşenlerin kan yağ dengesini iyileştirmeye ve damarların tıkanmasını önlemeye yardımcı olduğunu ortaya koymuştur. Ayrıca, kan basıncını düşürmeye yardımcı olarak kalp hastalıkları riskini azaltır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-14T19:31:14.461030Z",
     "iopub.status.busy": "2023-12-14T19:31:14.459946Z",
     "iopub.status.idle": "2023-12-14T19:31:14.969285Z",
     "shell.execute_reply": "2023-12-14T19:31:14.968076Z",
     "shell.execute_reply.started": "2023-12-14T19:31:14.460987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/playground-series-s3e14/sample_submission.csv\n",
      "/kaggle/input/playground-series-s3e14/train.csv\n",
      "/kaggle/input/playground-series-s3e14/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:32:35.617719Z",
     "iopub.status.busy": "2023-12-14T19:32:35.615966Z",
     "iopub.status.idle": "2023-12-14T19:33:49.119798Z",
     "shell.execute_reply": "2023-12-14T19:33:49.118191Z",
     "shell.execute_reply.started": "2023-12-14T19:32:35.617655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycaret\n",
      "  Obtaining dependency information for pycaret from https://files.pythonhosted.org/packages/eb/43/ec8d59a663e0a1a67196b404ec38ccb0051708bad74a48c80d96c61dd0e5/pycaret-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading pycaret-3.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: category-encoders>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (2.6.3)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from pycaret) (2.2.1)\n",
      "Requirement already satisfied: deprecation>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (2.1.0)\n",
      "Requirement already satisfied: imbalanced-learn>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from pycaret) (0.11.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.12.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (6.8.0)\n",
      "Requirement already satisfied: ipython>=5.5.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (8.14.0)\n",
      "Requirement already satisfied: ipywidgets>=7.6.5 in /opt/conda/lib/python3.10/site-packages (from pycaret) (7.7.1)\n",
      "Requirement already satisfied: jinja2>=1.2 in /opt/conda/lib/python3.10/site-packages (from pycaret) (3.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (1.3.2)\n",
      "Collecting kaleido>=0.2.1 (from pycaret)\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lightgbm>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (3.3.2)\n",
      "Requirement already satisfied: markupsafe>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from pycaret) (2.1.3)\n",
      "Collecting matplotlib<=3.6,>=3.3.0 (from pycaret)\n",
      "  Downloading matplotlib-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (5.9.2)\n",
      "Requirement already satisfied: numba>=0.55.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (0.57.1)\n",
      "Requirement already satisfied: numpy<1.27,>=1.21 in /opt/conda/lib/python3.10/site-packages (from pycaret) (1.24.3)\n",
      "Collecting pandas<2.0.0,>=1.3.0 (from pycaret)\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting plotly-resampler>=0.8.3.1 (from pycaret)\n",
      "  Obtaining dependency information for plotly-resampler>=0.8.3.1 from https://files.pythonhosted.org/packages/08/1d/87d4ed45c26226630bcb0a205ff006c00645cc68977e22c0f6f16a7f5d2b/plotly_resampler-0.9.1-py3-none-any.whl.metadata\n",
      "  Downloading plotly_resampler-0.9.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: plotly>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (5.16.1)\n",
      "Collecting pmdarima!=1.8.1,<3.0.0,>=1.8.0 (from pycaret)\n",
      "  Obtaining dependency information for pmdarima!=1.8.1,<3.0.0,>=1.8.0 from https://files.pythonhosted.org/packages/ec/2b/e7d18360d56396b62781ba4616527af49244d4bed51f0780646fa3953cc8/pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: psutil>=5.9.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (5.9.3)\n",
      "Collecting pyod>=1.0.8 (from pycaret)\n",
      "  Downloading pyod-1.1.2.tar.gz (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.27.1 in /opt/conda/lib/python3.10/site-packages (from pycaret) (2.31.0)\n",
      "Collecting schemdraw==0.15 (from pycaret)\n",
      "  Downloading schemdraw-0.15-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<1.3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (1.2.2)\n",
      "Requirement already satisfied: scikit-plot>=0.3.7 in /opt/conda/lib/python3.10/site-packages (from pycaret) (0.3.7)\n",
      "Collecting scipy~=1.10.1 (from pycaret)\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sktime!=0.17.1,!=0.17.2,!=0.18.0,<0.22.0,>=0.16.1 (from pycaret)\n",
      "  Obtaining dependency information for sktime!=0.17.1,!=0.17.2,!=0.18.0,<0.22.0,>=0.16.1 from https://files.pythonhosted.org/packages/0f/70/a791db68c6f6922eb9e79baf1466263147a487c5b7ba7813f5868901e032/sktime-0.21.1-py3-none-any.whl.metadata\n",
      "  Downloading sktime-0.21.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: statsmodels>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from pycaret) (0.14.0)\n",
      "Collecting tbats>=1.1.3 (from pycaret)\n",
      "  Downloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.0 in /opt/conda/lib/python3.10/site-packages (from pycaret) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from pycaret) (3.4.1)\n",
      "Requirement already satisfied: yellowbrick>=1.4 in /opt/conda/lib/python3.10/site-packages (from pycaret) (1.5)\n",
      "Requirement already satisfied: wurlitzer in /opt/conda/lib/python3.10/site-packages (from pycaret) (3.0.3)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category-encoders>=2.4.0->pycaret) (0.5.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from deprecation>=2.1.0->pycaret) (21.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn>=0.8.1->pycaret) (3.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.12.0->pycaret) (3.16.2)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (5.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.5.0->pycaret) (4.8.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=7.6.5->pycaret) (6.25.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=7.6.5->pycaret) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=7.6.5->pycaret) (3.6.6)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=7.6.5->pycaret) (3.0.8)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm>=3.0.0->pycaret) (0.41.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<=3.6,>=3.3.0->pycaret) (2.8.2)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.2.0->pycaret) (2.18.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.2.0->pycaret) (4.19.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.2.0->pycaret) (5.3.1)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.55.0->pycaret) (0.40.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.0.0,>=1.3.0->pycaret) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=5.0.0->pycaret) (8.2.3)\n",
      "Collecting dash<3.0.0,>=2.11.0 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Obtaining dependency information for dash<3.0.0,>=2.11.0 from https://files.pythonhosted.org/packages/7b/62/438626ab498869249e99376684ba6d88931e7c2dd86d03a92d9d7a9b4c54/dash-2.14.2-py3-none-any.whl.metadata\n",
      "  Downloading dash-2.14.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from plotly-resampler>=0.8.3.1->pycaret) (3.9.5)\n",
      "Collecting trace-updater>=0.0.8 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading trace_updater-0.0.9.1-py3-none-any.whl (185 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.2/185.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tsdownsample==0.1.2 (from plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading tsdownsample-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /opt/conda/lib/python3.10/site-packages (from pmdarima!=1.8.1,<3.0.0,>=1.8.0->pycaret) (3.0.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from pmdarima!=1.8.1,<3.0.0,>=1.8.0->pycaret) (1.26.15)\n",
      "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /opt/conda/lib/python3.10/site-packages (from pmdarima!=1.8.1,<3.0.0,>=1.8.0->pycaret) (68.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from pyod>=1.0.8->pycaret) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.27.1->pycaret) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.27.1->pycaret) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.27.1->pycaret) (2023.11.17)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /opt/conda/lib/python3.10/site-packages (from sktime!=0.17.1,!=0.17.2,!=0.18.0,<0.22.0,>=0.16.1->pycaret) (1.2.14)\n",
      "Collecting scikit-base<0.6.0 (from sktime!=0.17.1,!=0.17.2,!=0.18.0,<0.22.0,>=0.16.1->pycaret)\n",
      "  Obtaining dependency information for scikit-base<0.6.0 from https://files.pythonhosted.org/packages/e1/05/c42d5834896d0c112e76bb5b1ff0532e06cda405ae3154be5593a4aa8f37/scikit_base-0.5.2-py3-none-any.whl.metadata\n",
      "  Downloading scikit_base-0.5.2-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in /opt/conda/lib/python3.10/site-packages (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (3.0.0)\n",
      "Requirement already satisfied: Werkzeug<3.1 in /opt/conda/lib/python3.10/site-packages (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (3.0.1)\n",
      "Collecting dash-html-components==2.0.0 (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
      "Collecting dash-core-components==2.0.0 (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
      "Collecting dash-table==5.0.0 (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (4.5.0)\n",
      "Requirement already satisfied: retrying in /opt/conda/lib/python3.10/site-packages (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (1.3.3)\n",
      "Collecting ansi2html (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret)\n",
      "  Obtaining dependency information for ansi2html from https://files.pythonhosted.org/packages/42/d7/1bc3433a2406b891182c25a7926abe3eb19af5870d540375ebf7795885b3/ansi2html-1.9.1-py3-none-any.whl.metadata\n",
      "  Downloading ansi2html-1.9.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (1.5.6)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.13->sktime!=0.17.1,!=0.17.2,!=0.18.0,<0.22.0,>=0.16.1->pycaret) (1.15.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (0.1.4)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (1.6.7.post1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (7.4.9)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.3.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=5.5.0->pycaret) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (0.9.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core->nbformat>=4.2.0->pycaret) (4.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=5.5.0->pycaret) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.5.0->pycaret) (0.2.6)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.5.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.5.0->pycaret) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.5.0->pycaret) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.5.0->pycaret) (0.2.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.11.0->plotly-resampler>=0.8.3.1->pycaret) (1.7.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (0.4)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (21.3.0)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.4.5)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.17.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.17.1)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.0.0)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.12.1)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.2.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.2.2)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.5.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.6.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (4.12.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.5.13)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (21.2.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (3.7.1)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.4.4)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.5.0)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.6.2)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.5.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (1.1.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.21)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.5->pycaret) (0.1.1)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (2.0)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (1.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema>=2.6->nbformat>=4.2.0->pycaret) (1.2.3)\n",
      "Downloading pycaret-3.2.0-py3-none-any.whl (484 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.7/484.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading plotly_resampler-0.9.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sktime-0.21.1-py3-none-any.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dash-2.14.2-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_base-0.5.2-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ansi2html-1.9.1-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: pyod\n",
      "  Building wheel for pyod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyod: filename=pyod-1.1.2-py3-none-any.whl size=190289 sha256=79861ba456ade625024730ff1d97f26b441b62d571e117047ddafae7bd5932ba\n",
      "  Stored in directory: /root/.cache/pip/wheels/81/1b/61/aa85b78c3c0c8871f4231e3f4a03bb23cecb7db829498380ee\n",
      "Successfully built pyod\n",
      "Installing collected packages: trace-updater, kaleido, dash-table, dash-html-components, dash-core-components, tsdownsample, scipy, scikit-base, schemdraw, ansi2html, pandas, matplotlib, sktime, pyod, dash, pmdarima, plotly-resampler, tbats, pycaret\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.4\n",
      "    Uninstalling scipy-1.11.4:\n",
      "      Successfully uninstalled scipy-1.11.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.4\n",
      "    Uninstalling matplotlib-3.7.4:\n",
      "      Successfully uninstalled matplotlib-3.7.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\n",
      "beatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\n",
      "fitter 1.6.0 requires matplotlib>=3.7.2, but you have matplotlib 3.6.0 which is incompatible.\n",
      "fitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 1.5.3 which is incompatible.\n",
      "kaggle-environments 1.14.3 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.1 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ansi2html-1.9.1 dash-2.14.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 kaleido-0.2.1 matplotlib-3.6.0 pandas-1.5.3 plotly-resampler-0.9.1 pmdarima-2.0.4 pycaret-3.2.0 pyod-1.1.2 schemdraw-0.15 scikit-base-0.5.2 scipy-1.10.1 sktime-0.21.1 tbats-1.1.3 trace-updater-0.0.9.1 tsdownsample-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:33:49.123536Z",
     "iopub.status.busy": "2023-12-14T19:33:49.122872Z",
     "iopub.status.idle": "2023-12-14T19:33:54.175663Z",
     "shell.execute_reply": "2023-12-14T19:33:54.174020Z",
     "shell.execute_reply.started": "2023-12-14T19:33:49.123475Z"
    }
   },
   "outputs": [],
   "source": [
    "from pycaret.regression import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:34:45.723865Z",
     "iopub.status.busy": "2023-12-14T19:34:45.723386Z",
     "iopub.status.idle": "2023-12-14T19:34:45.836570Z",
     "shell.execute_reply": "2023-12-14T19:34:45.835467Z",
     "shell.execute_reply.started": "2023-12-14T19:34:45.723829Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/kaggle/input/playground-series-s3e14/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:34:48.271384Z",
     "iopub.status.busy": "2023-12-14T19:34:48.269984Z",
     "iopub.status.idle": "2023-12-14T19:34:48.318613Z",
     "shell.execute_reply": "2023-12-14T19:34:48.317749Z",
     "shell.execute_reply.started": "2023-12-14T19:34:48.271311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clonesize</th>\n",
       "      <th>honeybee</th>\n",
       "      <th>bumbles</th>\n",
       "      <th>andrena</th>\n",
       "      <th>osmia</th>\n",
       "      <th>MaxOfUpperTRange</th>\n",
       "      <th>MinOfUpperTRange</th>\n",
       "      <th>AverageOfUpperTRange</th>\n",
       "      <th>MaxOfLowerTRange</th>\n",
       "      <th>MinOfLowerTRange</th>\n",
       "      <th>AverageOfLowerTRange</th>\n",
       "      <th>RainingDays</th>\n",
       "      <th>AverageRainingDays</th>\n",
       "      <th>fruitset</th>\n",
       "      <th>fruitmass</th>\n",
       "      <th>seeds</th>\n",
       "      <th>yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>69.7</td>\n",
       "      <td>42.1</td>\n",
       "      <td>58.2</td>\n",
       "      <td>50.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>41.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.425011</td>\n",
       "      <td>0.417545</td>\n",
       "      <td>32.460887</td>\n",
       "      <td>4476.81146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>69.7</td>\n",
       "      <td>42.1</td>\n",
       "      <td>58.2</td>\n",
       "      <td>50.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>41.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.444908</td>\n",
       "      <td>0.422051</td>\n",
       "      <td>33.858317</td>\n",
       "      <td>5548.12201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>86.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>71.9</td>\n",
       "      <td>62.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.552927</td>\n",
       "      <td>0.470853</td>\n",
       "      <td>38.341781</td>\n",
       "      <td>6869.77760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.50</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.565976</td>\n",
       "      <td>0.478137</td>\n",
       "      <td>39.467561</td>\n",
       "      <td>6880.77590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.579677</td>\n",
       "      <td>0.494165</td>\n",
       "      <td>40.484512</td>\n",
       "      <td>7479.93417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15284</th>\n",
       "      <td>15284</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.556302</td>\n",
       "      <td>0.476308</td>\n",
       "      <td>40.546480</td>\n",
       "      <td>7667.83619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15285</th>\n",
       "      <td>15285</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>86.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>71.9</td>\n",
       "      <td>62.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.354413</td>\n",
       "      <td>0.388145</td>\n",
       "      <td>29.467434</td>\n",
       "      <td>3680.56025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15286</th>\n",
       "      <td>15286</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.75</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.422548</td>\n",
       "      <td>0.416786</td>\n",
       "      <td>32.299059</td>\n",
       "      <td>4696.44394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15287</th>\n",
       "      <td>15287</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>69.7</td>\n",
       "      <td>42.1</td>\n",
       "      <td>58.2</td>\n",
       "      <td>50.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>41.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.542170</td>\n",
       "      <td>0.434133</td>\n",
       "      <td>36.674243</td>\n",
       "      <td>6772.93347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15288</th>\n",
       "      <td>15288</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.50</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.492077</td>\n",
       "      <td>0.446576</td>\n",
       "      <td>35.094733</td>\n",
       "      <td>5867.99722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15289 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  clonesize  honeybee  bumbles  andrena  osmia  MaxOfUpperTRange   \n",
       "0          0       25.0      0.50     0.25     0.75   0.50              69.7  \\\n",
       "1          1       25.0      0.50     0.25     0.50   0.50              69.7   \n",
       "2          2       12.5      0.25     0.25     0.63   0.63              86.0   \n",
       "3          3       12.5      0.25     0.25     0.63   0.50              77.4   \n",
       "4          4       25.0      0.50     0.25     0.63   0.63              77.4   \n",
       "...      ...        ...       ...      ...      ...    ...               ...   \n",
       "15284  15284       12.5      0.25     0.25     0.38   0.50              77.4   \n",
       "15285  15285       12.5      0.25     0.25     0.25   0.50              86.0   \n",
       "15286  15286       25.0      0.50     0.25     0.38   0.75              77.4   \n",
       "15287  15287       25.0      0.50     0.25     0.63   0.63              69.7   \n",
       "15288  15288       25.0      0.50     0.25     0.63   0.50              77.4   \n",
       "\n",
       "       MinOfUpperTRange  AverageOfUpperTRange  MaxOfLowerTRange   \n",
       "0                  42.1                  58.2              50.2  \\\n",
       "1                  42.1                  58.2              50.2   \n",
       "2                  52.0                  71.9              62.0   \n",
       "3                  46.8                  64.7              55.8   \n",
       "4                  46.8                  64.7              55.8   \n",
       "...                 ...                   ...               ...   \n",
       "15284              46.8                  64.7              55.8   \n",
       "15285              52.0                  71.9              62.0   \n",
       "15286              46.8                  64.7              55.8   \n",
       "15287              42.1                  58.2              50.2   \n",
       "15288              46.8                  64.7              55.8   \n",
       "\n",
       "       MinOfLowerTRange  AverageOfLowerTRange  RainingDays   \n",
       "0                  24.3                  41.2         24.0  \\\n",
       "1                  24.3                  41.2         24.0   \n",
       "2                  30.0                  50.8         24.0   \n",
       "3                  27.0                  45.8         24.0   \n",
       "4                  27.0                  45.8         24.0   \n",
       "...                 ...                   ...          ...   \n",
       "15284              27.0                  45.8         16.0   \n",
       "15285              30.0                  50.8         34.0   \n",
       "15286              27.0                  45.8         34.0   \n",
       "15287              24.3                  41.2         24.0   \n",
       "15288              27.0                  45.8         16.0   \n",
       "\n",
       "       AverageRainingDays  fruitset  fruitmass      seeds       yield  \n",
       "0                    0.39  0.425011   0.417545  32.460887  4476.81146  \n",
       "1                    0.39  0.444908   0.422051  33.858317  5548.12201  \n",
       "2                    0.39  0.552927   0.470853  38.341781  6869.77760  \n",
       "3                    0.39  0.565976   0.478137  39.467561  6880.77590  \n",
       "4                    0.39  0.579677   0.494165  40.484512  7479.93417  \n",
       "...                   ...       ...        ...        ...         ...  \n",
       "15284                0.26  0.556302   0.476308  40.546480  7667.83619  \n",
       "15285                0.56  0.354413   0.388145  29.467434  3680.56025  \n",
       "15286                0.56  0.422548   0.416786  32.299059  4696.44394  \n",
       "15287                0.39  0.542170   0.434133  36.674243  6772.93347  \n",
       "15288                0.26  0.492077   0.446576  35.094733  5867.99722  \n",
       "\n",
       "[15289 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:35:06.593785Z",
     "iopub.status.busy": "2023-12-14T19:35:06.593264Z",
     "iopub.status.idle": "2023-12-14T19:35:06.606465Z",
     "shell.execute_reply": "2023-12-14T19:35:06.605395Z",
     "shell.execute_reply.started": "2023-12-14T19:35:06.593738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      0\n",
       "clonesize               0\n",
       "honeybee                0\n",
       "bumbles                 0\n",
       "andrena                 0\n",
       "osmia                   0\n",
       "MaxOfUpperTRange        0\n",
       "MinOfUpperTRange        0\n",
       "AverageOfUpperTRange    0\n",
       "MaxOfLowerTRange        0\n",
       "MinOfLowerTRange        0\n",
       "AverageOfLowerTRange    0\n",
       "RainingDays             0\n",
       "AverageRainingDays      0\n",
       "fruitset                0\n",
       "fruitmass               0\n",
       "seeds                   0\n",
       "yield                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:37:11.338866Z",
     "iopub.status.busy": "2023-12-14T19:37:11.337637Z",
     "iopub.status.idle": "2023-12-14T19:37:16.538909Z",
     "shell.execute_reply": "2023-12-14T19:37:16.537742Z",
     "shell.execute_reply.started": "2023-12-14T19:37:11.338817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_02938_row8_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_02938\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_02938_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_02938_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_02938_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_02938_row0_col1\" class=\"data row0 col1\" >5825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_02938_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_02938_row1_col1\" class=\"data row1 col1\" >yield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_02938_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_02938_row2_col1\" class=\"data row2 col1\" >Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_02938_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_02938_row3_col1\" class=\"data row3 col1\" >(15289, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_02938_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_02938_row4_col1\" class=\"data row4 col1\" >(15289, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_02938_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_02938_row5_col1\" class=\"data row5 col1\" >(10702, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_02938_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_02938_row6_col1\" class=\"data row6 col1\" >(4587, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_02938_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_02938_row7_col1\" class=\"data row7 col1\" >17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_02938_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n",
       "      <td id=\"T_02938_row8_col1\" class=\"data row8 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_02938_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n",
       "      <td id=\"T_02938_row9_col1\" class=\"data row9 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_02938_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_02938_row10_col1\" class=\"data row10 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_02938_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_02938_row11_col1\" class=\"data row11 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_02938_row12_col0\" class=\"data row12 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_02938_row12_col1\" class=\"data row12 col1\" >KFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_02938_row13_col0\" class=\"data row13 col0\" >Fold Number</td>\n",
       "      <td id=\"T_02938_row13_col1\" class=\"data row13 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_02938_row14_col0\" class=\"data row14 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_02938_row14_col1\" class=\"data row14 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_02938_row15_col0\" class=\"data row15 col0\" >Use GPU</td>\n",
       "      <td id=\"T_02938_row15_col1\" class=\"data row15 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_02938_row16_col0\" class=\"data row16 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_02938_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_02938_row17_col0\" class=\"data row17 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_02938_row17_col1\" class=\"data row17 col1\" >reg-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02938_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_02938_row18_col0\" class=\"data row18 col0\" >USI</td>\n",
       "      <td id=\"T_02938_row18_col1\" class=\"data row18 col1\" >0037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a80841e4160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pycaret.regression.oop.RegressionExperiment at 0x7a808592bcd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup(data=df,target='yield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:37:28.632938Z",
     "iopub.status.busy": "2023-12-14T19:37:28.632461Z",
     "iopub.status.idle": "2023-12-14T19:39:28.445783Z",
     "shell.execute_reply": "2023-12-14T19:39:28.444439Z",
     "shell.execute_reply.started": "2023-12-14T19:37:28.632897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ca901 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_ca901_row0_col0, #T_ca901_row1_col0, #T_ca901_row1_col1, #T_ca901_row1_col2, #T_ca901_row1_col3, #T_ca901_row1_col4, #T_ca901_row1_col5, #T_ca901_row1_col6, #T_ca901_row2_col0, #T_ca901_row2_col1, #T_ca901_row2_col2, #T_ca901_row2_col3, #T_ca901_row2_col4, #T_ca901_row2_col5, #T_ca901_row2_col6, #T_ca901_row3_col0, #T_ca901_row3_col1, #T_ca901_row3_col2, #T_ca901_row3_col3, #T_ca901_row3_col4, #T_ca901_row3_col5, #T_ca901_row3_col6, #T_ca901_row4_col0, #T_ca901_row4_col1, #T_ca901_row4_col2, #T_ca901_row4_col3, #T_ca901_row4_col4, #T_ca901_row4_col5, #T_ca901_row4_col6, #T_ca901_row5_col0, #T_ca901_row5_col1, #T_ca901_row5_col2, #T_ca901_row5_col3, #T_ca901_row5_col4, #T_ca901_row5_col5, #T_ca901_row5_col6, #T_ca901_row6_col0, #T_ca901_row6_col1, #T_ca901_row6_col2, #T_ca901_row6_col3, #T_ca901_row6_col4, #T_ca901_row6_col5, #T_ca901_row6_col6, #T_ca901_row7_col0, #T_ca901_row7_col1, #T_ca901_row7_col2, #T_ca901_row7_col3, #T_ca901_row7_col4, #T_ca901_row7_col5, #T_ca901_row7_col6, #T_ca901_row8_col0, #T_ca901_row8_col1, #T_ca901_row8_col2, #T_ca901_row8_col3, #T_ca901_row8_col4, #T_ca901_row8_col5, #T_ca901_row8_col6, #T_ca901_row9_col0, #T_ca901_row9_col1, #T_ca901_row9_col2, #T_ca901_row9_col3, #T_ca901_row9_col4, #T_ca901_row9_col5, #T_ca901_row9_col6, #T_ca901_row10_col0, #T_ca901_row10_col1, #T_ca901_row10_col2, #T_ca901_row10_col3, #T_ca901_row10_col4, #T_ca901_row10_col5, #T_ca901_row10_col6, #T_ca901_row11_col0, #T_ca901_row11_col1, #T_ca901_row11_col2, #T_ca901_row11_col3, #T_ca901_row11_col4, #T_ca901_row11_col5, #T_ca901_row11_col6, #T_ca901_row12_col0, #T_ca901_row12_col1, #T_ca901_row12_col2, #T_ca901_row12_col3, #T_ca901_row12_col4, #T_ca901_row12_col5, #T_ca901_row12_col6, #T_ca901_row13_col0, #T_ca901_row13_col1, #T_ca901_row13_col2, #T_ca901_row13_col3, #T_ca901_row13_col4, #T_ca901_row13_col5, #T_ca901_row13_col6, #T_ca901_row14_col0, #T_ca901_row14_col1, #T_ca901_row14_col2, #T_ca901_row14_col3, #T_ca901_row14_col4, #T_ca901_row14_col5, #T_ca901_row14_col6, #T_ca901_row15_col0, #T_ca901_row15_col1, #T_ca901_row15_col2, #T_ca901_row15_col3, #T_ca901_row15_col4, #T_ca901_row15_col5, #T_ca901_row15_col6, #T_ca901_row16_col0, #T_ca901_row16_col1, #T_ca901_row16_col2, #T_ca901_row16_col3, #T_ca901_row16_col4, #T_ca901_row16_col5, #T_ca901_row16_col6, #T_ca901_row17_col0, #T_ca901_row17_col1, #T_ca901_row17_col2, #T_ca901_row17_col3, #T_ca901_row17_col4, #T_ca901_row17_col5, #T_ca901_row17_col6, #T_ca901_row18_col0, #T_ca901_row18_col1, #T_ca901_row18_col2, #T_ca901_row18_col3, #T_ca901_row18_col4, #T_ca901_row18_col5, #T_ca901_row18_col6, #T_ca901_row19_col0, #T_ca901_row19_col1, #T_ca901_row19_col2, #T_ca901_row19_col3, #T_ca901_row19_col4, #T_ca901_row19_col5, #T_ca901_row19_col6 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_ca901_row0_col1, #T_ca901_row0_col2, #T_ca901_row0_col3, #T_ca901_row0_col4, #T_ca901_row0_col5, #T_ca901_row0_col6 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_ca901_row0_col7, #T_ca901_row1_col7, #T_ca901_row2_col7, #T_ca901_row3_col7, #T_ca901_row4_col7, #T_ca901_row5_col7, #T_ca901_row6_col7, #T_ca901_row7_col7, #T_ca901_row9_col7, #T_ca901_row10_col7, #T_ca901_row11_col7, #T_ca901_row12_col7, #T_ca901_row13_col7, #T_ca901_row14_col7, #T_ca901_row15_col7, #T_ca901_row16_col7, #T_ca901_row17_col7, #T_ca901_row19_col7 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_ca901_row8_col7, #T_ca901_row18_col7 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ca901\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ca901_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_ca901_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_ca901_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_ca901_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th id=\"T_ca901_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th id=\"T_ca901_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th id=\"T_ca901_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
       "      <th id=\"T_ca901_level0_col7\" class=\"col_heading level0 col7\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row0\" class=\"row_heading level0 row0\" >gbr</th>\n",
       "      <td id=\"T_ca901_row0_col0\" class=\"data row0 col0\" >Gradient Boosting Regressor</td>\n",
       "      <td id=\"T_ca901_row0_col1\" class=\"data row0 col1\" >357.8059</td>\n",
       "      <td id=\"T_ca901_row0_col2\" class=\"data row0 col2\" >329037.0563</td>\n",
       "      <td id=\"T_ca901_row0_col3\" class=\"data row0 col3\" >573.2012</td>\n",
       "      <td id=\"T_ca901_row0_col4\" class=\"data row0 col4\" >0.8173</td>\n",
       "      <td id=\"T_ca901_row0_col5\" class=\"data row0 col5\" >0.1052</td>\n",
       "      <td id=\"T_ca901_row0_col6\" class=\"data row0 col6\" >0.0639</td>\n",
       "      <td id=\"T_ca901_row0_col7\" class=\"data row0 col7\" >0.7010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row1\" class=\"row_heading level0 row1\" >lightgbm</th>\n",
       "      <td id=\"T_ca901_row1_col0\" class=\"data row1 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_ca901_row1_col1\" class=\"data row1 col1\" >360.5625</td>\n",
       "      <td id=\"T_ca901_row1_col2\" class=\"data row1 col2\" >331798.7964</td>\n",
       "      <td id=\"T_ca901_row1_col3\" class=\"data row1 col3\" >575.5237</td>\n",
       "      <td id=\"T_ca901_row1_col4\" class=\"data row1 col4\" >0.8159</td>\n",
       "      <td id=\"T_ca901_row1_col5\" class=\"data row1 col5\" >0.1062</td>\n",
       "      <td id=\"T_ca901_row1_col6\" class=\"data row1 col6\" >0.0646</td>\n",
       "      <td id=\"T_ca901_row1_col7\" class=\"data row1 col7\" >0.4840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row2\" class=\"row_heading level0 row2\" >catboost</th>\n",
       "      <td id=\"T_ca901_row2_col0\" class=\"data row2 col0\" >CatBoost Regressor</td>\n",
       "      <td id=\"T_ca901_row2_col1\" class=\"data row2 col1\" >364.6832</td>\n",
       "      <td id=\"T_ca901_row2_col2\" class=\"data row2 col2\" >335996.9709</td>\n",
       "      <td id=\"T_ca901_row2_col3\" class=\"data row2 col3\" >579.1927</td>\n",
       "      <td id=\"T_ca901_row2_col4\" class=\"data row2 col4\" >0.8135</td>\n",
       "      <td id=\"T_ca901_row2_col5\" class=\"data row2 col5\" >0.1070</td>\n",
       "      <td id=\"T_ca901_row2_col6\" class=\"data row2 col6\" >0.0655</td>\n",
       "      <td id=\"T_ca901_row2_col7\" class=\"data row2 col7\" >4.0560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row3\" class=\"row_heading level0 row3\" >rf</th>\n",
       "      <td id=\"T_ca901_row3_col0\" class=\"data row3 col0\" >Random Forest Regressor</td>\n",
       "      <td id=\"T_ca901_row3_col1\" class=\"data row3 col1\" >369.5647</td>\n",
       "      <td id=\"T_ca901_row3_col2\" class=\"data row3 col2\" >346744.6036</td>\n",
       "      <td id=\"T_ca901_row3_col3\" class=\"data row3 col3\" >588.3309</td>\n",
       "      <td id=\"T_ca901_row3_col4\" class=\"data row3 col4\" >0.8075</td>\n",
       "      <td id=\"T_ca901_row3_col5\" class=\"data row3 col5\" >0.1076</td>\n",
       "      <td id=\"T_ca901_row3_col6\" class=\"data row3 col6\" >0.0660</td>\n",
       "      <td id=\"T_ca901_row3_col7\" class=\"data row3 col7\" >2.6050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row4\" class=\"row_heading level0 row4\" >ridge</th>\n",
       "      <td id=\"T_ca901_row4_col0\" class=\"data row4 col0\" >Ridge Regression</td>\n",
       "      <td id=\"T_ca901_row4_col1\" class=\"data row4 col1\" >376.9949</td>\n",
       "      <td id=\"T_ca901_row4_col2\" class=\"data row4 col2\" >349828.9377</td>\n",
       "      <td id=\"T_ca901_row4_col3\" class=\"data row4 col3\" >591.0679</td>\n",
       "      <td id=\"T_ca901_row4_col4\" class=\"data row4 col4\" >0.8058</td>\n",
       "      <td id=\"T_ca901_row4_col5\" class=\"data row4 col5\" >0.1090</td>\n",
       "      <td id=\"T_ca901_row4_col6\" class=\"data row4 col6\" >0.0680</td>\n",
       "      <td id=\"T_ca901_row4_col7\" class=\"data row4 col7\" >0.0220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row5\" class=\"row_heading level0 row5\" >et</th>\n",
       "      <td id=\"T_ca901_row5_col0\" class=\"data row5 col0\" >Extra Trees Regressor</td>\n",
       "      <td id=\"T_ca901_row5_col1\" class=\"data row5 col1\" >380.8259</td>\n",
       "      <td id=\"T_ca901_row5_col2\" class=\"data row5 col2\" >354897.4463</td>\n",
       "      <td id=\"T_ca901_row5_col3\" class=\"data row5 col3\" >595.2325</td>\n",
       "      <td id=\"T_ca901_row5_col4\" class=\"data row5 col4\" >0.8031</td>\n",
       "      <td id=\"T_ca901_row5_col5\" class=\"data row5 col5\" >0.1091</td>\n",
       "      <td id=\"T_ca901_row5_col6\" class=\"data row5 col6\" >0.0680</td>\n",
       "      <td id=\"T_ca901_row5_col7\" class=\"data row5 col7\" >1.6020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row6\" class=\"row_heading level0 row6\" >br</th>\n",
       "      <td id=\"T_ca901_row6_col0\" class=\"data row6 col0\" >Bayesian Ridge</td>\n",
       "      <td id=\"T_ca901_row6_col1\" class=\"data row6 col1\" >374.2334</td>\n",
       "      <td id=\"T_ca901_row6_col2\" class=\"data row6 col2\" >355515.1823</td>\n",
       "      <td id=\"T_ca901_row6_col3\" class=\"data row6 col3\" >595.4332</td>\n",
       "      <td id=\"T_ca901_row6_col4\" class=\"data row6 col4\" >0.8025</td>\n",
       "      <td id=\"T_ca901_row6_col5\" class=\"data row6 col5\" >0.1085</td>\n",
       "      <td id=\"T_ca901_row6_col6\" class=\"data row6 col6\" >0.0672</td>\n",
       "      <td id=\"T_ca901_row6_col7\" class=\"data row6 col7\" >0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row7\" class=\"row_heading level0 row7\" >lasso</th>\n",
       "      <td id=\"T_ca901_row7_col0\" class=\"data row7 col0\" >Lasso Regression</td>\n",
       "      <td id=\"T_ca901_row7_col1\" class=\"data row7 col1\" >382.5134</td>\n",
       "      <td id=\"T_ca901_row7_col2\" class=\"data row7 col2\" >356094.9105</td>\n",
       "      <td id=\"T_ca901_row7_col3\" class=\"data row7 col3\" >596.3582</td>\n",
       "      <td id=\"T_ca901_row7_col4\" class=\"data row7 col4\" >0.8023</td>\n",
       "      <td id=\"T_ca901_row7_col5\" class=\"data row7 col5\" >0.1098</td>\n",
       "      <td id=\"T_ca901_row7_col6\" class=\"data row7 col6\" >0.0690</td>\n",
       "      <td id=\"T_ca901_row7_col7\" class=\"data row7 col7\" >0.1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row8\" class=\"row_heading level0 row8\" >llar</th>\n",
       "      <td id=\"T_ca901_row8_col0\" class=\"data row8 col0\" >Lasso Least Angle Regression</td>\n",
       "      <td id=\"T_ca901_row8_col1\" class=\"data row8 col1\" >382.5372</td>\n",
       "      <td id=\"T_ca901_row8_col2\" class=\"data row8 col2\" >356625.3183</td>\n",
       "      <td id=\"T_ca901_row8_col3\" class=\"data row8 col3\" >596.7995</td>\n",
       "      <td id=\"T_ca901_row8_col4\" class=\"data row8 col4\" >0.8020</td>\n",
       "      <td id=\"T_ca901_row8_col5\" class=\"data row8 col5\" >0.1099</td>\n",
       "      <td id=\"T_ca901_row8_col6\" class=\"data row8 col6\" >0.0690</td>\n",
       "      <td id=\"T_ca901_row8_col7\" class=\"data row8 col7\" >0.0210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row9\" class=\"row_heading level0 row9\" >lr</th>\n",
       "      <td id=\"T_ca901_row9_col0\" class=\"data row9 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_ca901_row9_col1\" class=\"data row9 col1\" >374.3896</td>\n",
       "      <td id=\"T_ca901_row9_col2\" class=\"data row9 col2\" >358100.7417</td>\n",
       "      <td id=\"T_ca901_row9_col3\" class=\"data row9 col3\" >597.3621</td>\n",
       "      <td id=\"T_ca901_row9_col4\" class=\"data row9 col4\" >0.8010</td>\n",
       "      <td id=\"T_ca901_row9_col5\" class=\"data row9 col5\" >0.1086</td>\n",
       "      <td id=\"T_ca901_row9_col6\" class=\"data row9 col6\" >0.0673</td>\n",
       "      <td id=\"T_ca901_row9_col7\" class=\"data row9 col7\" >0.4910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row10\" class=\"row_heading level0 row10\" >xgboost</th>\n",
       "      <td id=\"T_ca901_row10_col0\" class=\"data row10 col0\" >Extreme Gradient Boosting</td>\n",
       "      <td id=\"T_ca901_row10_col1\" class=\"data row10 col1\" >387.8126</td>\n",
       "      <td id=\"T_ca901_row10_col2\" class=\"data row10 col2\" >364795.4000</td>\n",
       "      <td id=\"T_ca901_row10_col3\" class=\"data row10 col3\" >603.5142</td>\n",
       "      <td id=\"T_ca901_row10_col4\" class=\"data row10 col4\" >0.7975</td>\n",
       "      <td id=\"T_ca901_row10_col5\" class=\"data row10 col5\" >0.1112</td>\n",
       "      <td id=\"T_ca901_row10_col6\" class=\"data row10 col6\" >0.0695</td>\n",
       "      <td id=\"T_ca901_row10_col7\" class=\"data row10 col7\" >0.1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row11\" class=\"row_heading level0 row11\" >en</th>\n",
       "      <td id=\"T_ca901_row11_col0\" class=\"data row11 col0\" >Elastic Net</td>\n",
       "      <td id=\"T_ca901_row11_col1\" class=\"data row11 col1\" >433.3183</td>\n",
       "      <td id=\"T_ca901_row11_col2\" class=\"data row11 col2\" >420470.0114</td>\n",
       "      <td id=\"T_ca901_row11_col3\" class=\"data row11 col3\" >648.1348</td>\n",
       "      <td id=\"T_ca901_row11_col4\" class=\"data row11 col4\" >0.7666</td>\n",
       "      <td id=\"T_ca901_row11_col5\" class=\"data row11 col5\" >0.1234</td>\n",
       "      <td id=\"T_ca901_row11_col6\" class=\"data row11 col6\" >0.0808</td>\n",
       "      <td id=\"T_ca901_row11_col7\" class=\"data row11 col7\" >0.0660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row12\" class=\"row_heading level0 row12\" >huber</th>\n",
       "      <td id=\"T_ca901_row12_col0\" class=\"data row12 col0\" >Huber Regressor</td>\n",
       "      <td id=\"T_ca901_row12_col1\" class=\"data row12 col1\" >467.7668</td>\n",
       "      <td id=\"T_ca901_row12_col2\" class=\"data row12 col2\" >452706.5328</td>\n",
       "      <td id=\"T_ca901_row12_col3\" class=\"data row12 col3\" >672.5021</td>\n",
       "      <td id=\"T_ca901_row12_col4\" class=\"data row12 col4\" >0.7487</td>\n",
       "      <td id=\"T_ca901_row12_col5\" class=\"data row12 col5\" >0.1285</td>\n",
       "      <td id=\"T_ca901_row12_col6\" class=\"data row12 col6\" >0.0876</td>\n",
       "      <td id=\"T_ca901_row12_col7\" class=\"data row12 col7\" >0.0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row13\" class=\"row_heading level0 row13\" >ada</th>\n",
       "      <td id=\"T_ca901_row13_col0\" class=\"data row13 col0\" >AdaBoost Regressor</td>\n",
       "      <td id=\"T_ca901_row13_col1\" class=\"data row13 col1\" >555.4272</td>\n",
       "      <td id=\"T_ca901_row13_col2\" class=\"data row13 col2\" >573734.8046</td>\n",
       "      <td id=\"T_ca901_row13_col3\" class=\"data row13 col3\" >756.4712</td>\n",
       "      <td id=\"T_ca901_row13_col4\" class=\"data row13 col4\" >0.6820</td>\n",
       "      <td id=\"T_ca901_row13_col5\" class=\"data row13 col5\" >0.1497</td>\n",
       "      <td id=\"T_ca901_row13_col6\" class=\"data row13 col6\" >0.1082</td>\n",
       "      <td id=\"T_ca901_row13_col7\" class=\"data row13 col7\" >0.1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row14\" class=\"row_heading level0 row14\" >dt</th>\n",
       "      <td id=\"T_ca901_row14_col0\" class=\"data row14 col0\" >Decision Tree Regressor</td>\n",
       "      <td id=\"T_ca901_row14_col1\" class=\"data row14 col1\" >537.2560</td>\n",
       "      <td id=\"T_ca901_row14_col2\" class=\"data row14 col2\" >700761.6752</td>\n",
       "      <td id=\"T_ca901_row14_col3\" class=\"data row14 col3\" >836.2740</td>\n",
       "      <td id=\"T_ca901_row14_col4\" class=\"data row14 col4\" >0.6110</td>\n",
       "      <td id=\"T_ca901_row14_col5\" class=\"data row14 col5\" >0.1540</td>\n",
       "      <td id=\"T_ca901_row14_col6\" class=\"data row14 col6\" >0.0948</td>\n",
       "      <td id=\"T_ca901_row14_col7\" class=\"data row14 col7\" >0.0610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row15\" class=\"row_heading level0 row15\" >lar</th>\n",
       "      <td id=\"T_ca901_row15_col0\" class=\"data row15 col0\" >Least Angle Regression</td>\n",
       "      <td id=\"T_ca901_row15_col1\" class=\"data row15 col1\" >390.1981</td>\n",
       "      <td id=\"T_ca901_row15_col2\" class=\"data row15 col2\" >944092.3260</td>\n",
       "      <td id=\"T_ca901_row15_col3\" class=\"data row15 col3\" >788.2085</td>\n",
       "      <td id=\"T_ca901_row15_col4\" class=\"data row15 col4\" >0.4660</td>\n",
       "      <td id=\"T_ca901_row15_col5\" class=\"data row15 col5\" >0.1151</td>\n",
       "      <td id=\"T_ca901_row15_col6\" class=\"data row15 col6\" >0.0698</td>\n",
       "      <td id=\"T_ca901_row15_col7\" class=\"data row15 col7\" >0.0230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row16\" class=\"row_heading level0 row16\" >knn</th>\n",
       "      <td id=\"T_ca901_row16_col0\" class=\"data row16 col0\" >K Neighbors Regressor</td>\n",
       "      <td id=\"T_ca901_row16_col1\" class=\"data row16 col1\" >932.1974</td>\n",
       "      <td id=\"T_ca901_row16_col2\" class=\"data row16 col2\" >1395329.6250</td>\n",
       "      <td id=\"T_ca901_row16_col3\" class=\"data row16 col3\" >1181.0427</td>\n",
       "      <td id=\"T_ca901_row16_col4\" class=\"data row16 col4\" >0.2253</td>\n",
       "      <td id=\"T_ca901_row16_col5\" class=\"data row16 col5\" >0.2197</td>\n",
       "      <td id=\"T_ca901_row16_col6\" class=\"data row16 col6\" >0.1774</td>\n",
       "      <td id=\"T_ca901_row16_col7\" class=\"data row16 col7\" >0.0690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row17\" class=\"row_heading level0 row17\" >dummy</th>\n",
       "      <td id=\"T_ca901_row17_col0\" class=\"data row17 col0\" >Dummy Regressor</td>\n",
       "      <td id=\"T_ca901_row17_col1\" class=\"data row17 col1\" >1096.8354</td>\n",
       "      <td id=\"T_ca901_row17_col2\" class=\"data row17 col2\" >1805484.2875</td>\n",
       "      <td id=\"T_ca901_row17_col3\" class=\"data row17 col3\" >1343.4553</td>\n",
       "      <td id=\"T_ca901_row17_col4\" class=\"data row17 col4\" >-0.0022</td>\n",
       "      <td id=\"T_ca901_row17_col5\" class=\"data row17 col5\" >0.2480</td>\n",
       "      <td id=\"T_ca901_row17_col6\" class=\"data row17 col6\" >0.2100</td>\n",
       "      <td id=\"T_ca901_row17_col7\" class=\"data row17 col7\" >0.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row18\" class=\"row_heading level0 row18\" >omp</th>\n",
       "      <td id=\"T_ca901_row18_col0\" class=\"data row18 col0\" >Orthogonal Matching Pursuit</td>\n",
       "      <td id=\"T_ca901_row18_col1\" class=\"data row18 col1\" >1096.8709</td>\n",
       "      <td id=\"T_ca901_row18_col2\" class=\"data row18 col2\" >1805725.5592</td>\n",
       "      <td id=\"T_ca901_row18_col3\" class=\"data row18 col3\" >1343.5474</td>\n",
       "      <td id=\"T_ca901_row18_col4\" class=\"data row18 col4\" >-0.0023</td>\n",
       "      <td id=\"T_ca901_row18_col5\" class=\"data row18 col5\" >0.2480</td>\n",
       "      <td id=\"T_ca901_row18_col6\" class=\"data row18 col6\" >0.2100</td>\n",
       "      <td id=\"T_ca901_row18_col7\" class=\"data row18 col7\" >0.0210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ca901_level0_row19\" class=\"row_heading level0 row19\" >par</th>\n",
       "      <td id=\"T_ca901_row19_col0\" class=\"data row19 col0\" >Passive Aggressive Regressor</td>\n",
       "      <td id=\"T_ca901_row19_col1\" class=\"data row19 col1\" >1312.7470</td>\n",
       "      <td id=\"T_ca901_row19_col2\" class=\"data row19 col2\" >2981069.6567</td>\n",
       "      <td id=\"T_ca901_row19_col3\" class=\"data row19 col3\" >1583.6735</td>\n",
       "      <td id=\"T_ca901_row19_col4\" class=\"data row19 col4\" >-0.6259</td>\n",
       "      <td id=\"T_ca901_row19_col5\" class=\"data row19 col5\" >0.2818</td>\n",
       "      <td id=\"T_ca901_row19_col6\" class=\"data row19 col6\" >0.2433</td>\n",
       "      <td id=\"T_ca901_row19_col7\" class=\"data row19 col7\" >0.0310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a80843313f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model=compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTO ML İLE ÇÖZÜMDE EN YÜKSEK DOĞRULUK ORANI %81 OLARAK ÇIKTI\n",
    "#DEEP LEARNİN İLE DE ÇÖZMEYE ÇALIŞALIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clonesize</th>\n",
       "      <th>honeybee</th>\n",
       "      <th>bumbles</th>\n",
       "      <th>andrena</th>\n",
       "      <th>osmia</th>\n",
       "      <th>MaxOfUpperTRange</th>\n",
       "      <th>MinOfUpperTRange</th>\n",
       "      <th>AverageOfUpperTRange</th>\n",
       "      <th>MaxOfLowerTRange</th>\n",
       "      <th>MinOfLowerTRange</th>\n",
       "      <th>AverageOfLowerTRange</th>\n",
       "      <th>RainingDays</th>\n",
       "      <th>AverageRainingDays</th>\n",
       "      <th>fruitset</th>\n",
       "      <th>fruitmass</th>\n",
       "      <th>seeds</th>\n",
       "      <th>yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>69.7</td>\n",
       "      <td>42.1</td>\n",
       "      <td>58.2</td>\n",
       "      <td>50.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>41.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.425011</td>\n",
       "      <td>0.417545</td>\n",
       "      <td>32.460887</td>\n",
       "      <td>4476.81146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>69.7</td>\n",
       "      <td>42.1</td>\n",
       "      <td>58.2</td>\n",
       "      <td>50.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>41.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.444908</td>\n",
       "      <td>0.422051</td>\n",
       "      <td>33.858317</td>\n",
       "      <td>5548.12201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>86.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>71.9</td>\n",
       "      <td>62.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.552927</td>\n",
       "      <td>0.470853</td>\n",
       "      <td>38.341781</td>\n",
       "      <td>6869.77760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.50</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.565976</td>\n",
       "      <td>0.478137</td>\n",
       "      <td>39.467561</td>\n",
       "      <td>6880.77590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.579677</td>\n",
       "      <td>0.494165</td>\n",
       "      <td>40.484512</td>\n",
       "      <td>7479.93417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15284</th>\n",
       "      <td>15284</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.556302</td>\n",
       "      <td>0.476308</td>\n",
       "      <td>40.546480</td>\n",
       "      <td>7667.83619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15285</th>\n",
       "      <td>15285</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>86.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>71.9</td>\n",
       "      <td>62.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.354413</td>\n",
       "      <td>0.388145</td>\n",
       "      <td>29.467434</td>\n",
       "      <td>3680.56025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15286</th>\n",
       "      <td>15286</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.75</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.422548</td>\n",
       "      <td>0.416786</td>\n",
       "      <td>32.299059</td>\n",
       "      <td>4696.44394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15287</th>\n",
       "      <td>15287</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>69.7</td>\n",
       "      <td>42.1</td>\n",
       "      <td>58.2</td>\n",
       "      <td>50.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>41.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.542170</td>\n",
       "      <td>0.434133</td>\n",
       "      <td>36.674243</td>\n",
       "      <td>6772.93347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15288</th>\n",
       "      <td>15288</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.50</td>\n",
       "      <td>77.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>64.7</td>\n",
       "      <td>55.8</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.492077</td>\n",
       "      <td>0.446576</td>\n",
       "      <td>35.094733</td>\n",
       "      <td>5867.99722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15289 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  clonesize  honeybee  bumbles  andrena  osmia  MaxOfUpperTRange  \\\n",
       "0          0       25.0      0.50     0.25     0.75   0.50              69.7   \n",
       "1          1       25.0      0.50     0.25     0.50   0.50              69.7   \n",
       "2          2       12.5      0.25     0.25     0.63   0.63              86.0   \n",
       "3          3       12.5      0.25     0.25     0.63   0.50              77.4   \n",
       "4          4       25.0      0.50     0.25     0.63   0.63              77.4   \n",
       "...      ...        ...       ...      ...      ...    ...               ...   \n",
       "15284  15284       12.5      0.25     0.25     0.38   0.50              77.4   \n",
       "15285  15285       12.5      0.25     0.25     0.25   0.50              86.0   \n",
       "15286  15286       25.0      0.50     0.25     0.38   0.75              77.4   \n",
       "15287  15287       25.0      0.50     0.25     0.63   0.63              69.7   \n",
       "15288  15288       25.0      0.50     0.25     0.63   0.50              77.4   \n",
       "\n",
       "       MinOfUpperTRange  AverageOfUpperTRange  MaxOfLowerTRange  \\\n",
       "0                  42.1                  58.2              50.2   \n",
       "1                  42.1                  58.2              50.2   \n",
       "2                  52.0                  71.9              62.0   \n",
       "3                  46.8                  64.7              55.8   \n",
       "4                  46.8                  64.7              55.8   \n",
       "...                 ...                   ...               ...   \n",
       "15284              46.8                  64.7              55.8   \n",
       "15285              52.0                  71.9              62.0   \n",
       "15286              46.8                  64.7              55.8   \n",
       "15287              42.1                  58.2              50.2   \n",
       "15288              46.8                  64.7              55.8   \n",
       "\n",
       "       MinOfLowerTRange  AverageOfLowerTRange  RainingDays  \\\n",
       "0                  24.3                  41.2         24.0   \n",
       "1                  24.3                  41.2         24.0   \n",
       "2                  30.0                  50.8         24.0   \n",
       "3                  27.0                  45.8         24.0   \n",
       "4                  27.0                  45.8         24.0   \n",
       "...                 ...                   ...          ...   \n",
       "15284              27.0                  45.8         16.0   \n",
       "15285              30.0                  50.8         34.0   \n",
       "15286              27.0                  45.8         34.0   \n",
       "15287              24.3                  41.2         24.0   \n",
       "15288              27.0                  45.8         16.0   \n",
       "\n",
       "       AverageRainingDays  fruitset  fruitmass      seeds       yield  \n",
       "0                    0.39  0.425011   0.417545  32.460887  4476.81146  \n",
       "1                    0.39  0.444908   0.422051  33.858317  5548.12201  \n",
       "2                    0.39  0.552927   0.470853  38.341781  6869.77760  \n",
       "3                    0.39  0.565976   0.478137  39.467561  6880.77590  \n",
       "4                    0.39  0.579677   0.494165  40.484512  7479.93417  \n",
       "...                   ...       ...        ...        ...         ...  \n",
       "15284                0.26  0.556302   0.476308  40.546480  7667.83619  \n",
       "15285                0.56  0.354413   0.388145  29.467434  3680.56025  \n",
       "15286                0.56  0.422548   0.416786  32.299059  4696.44394  \n",
       "15287                0.39  0.542170   0.434133  36.674243  6772.93347  \n",
       "15288                0.26  0.492077   0.446576  35.094733  5867.99722  \n",
       "\n",
       "[15289 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:41:38.419093Z",
     "iopub.status.busy": "2023-12-14T19:41:38.418612Z",
     "iopub.status.idle": "2023-12-14T19:41:38.427920Z",
     "shell.execute_reply": "2023-12-14T19:41:38.426711Z",
     "shell.execute_reply.started": "2023-12-14T19:41:38.419044Z"
    }
   },
   "outputs": [],
   "source": [
    "x=data.drop('yield',axis=1)\n",
    "y=data[['yield']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:41:49.076152Z",
     "iopub.status.busy": "2023-12-14T19:41:49.075678Z",
     "iopub.status.idle": "2023-12-14T19:41:49.082285Z",
     "shell.execute_reply": "2023-12-14T19:41:49.080952Z",
     "shell.execute_reply.started": "2023-12-14T19:41:49.076119Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:41:59.803700Z",
     "iopub.status.busy": "2023-12-14T19:41:59.802502Z",
     "iopub.status.idle": "2023-12-14T19:41:59.814339Z",
     "shell.execute_reply": "2023-12-14T19:41:59.813115Z",
     "shell.execute_reply.started": "2023-12-14T19:41:59.803645Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:42:09.325563Z",
     "iopub.status.busy": "2023-12-14T19:42:09.324661Z",
     "iopub.status.idle": "2023-12-14T19:42:09.333434Z",
     "shell.execute_reply": "2023-12-14T19:42:09.332226Z",
     "shell.execute_reply.started": "2023-12-14T19:42:09.325521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15289, 18)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:42:55.776944Z",
     "iopub.status.busy": "2023-12-14T19:42:55.776496Z",
     "iopub.status.idle": "2023-12-14T19:43:08.466688Z",
     "shell.execute_reply": "2023-12-14T19:43:08.465495Z",
     "shell.execute_reply.started": "2023-12-14T19:42:55.776910Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:43:08.469822Z",
     "iopub.status.busy": "2023-12-14T19:43:08.468781Z",
     "iopub.status.idle": "2023-12-14T19:43:08.592984Z",
     "shell.execute_reply": "2023-12-14T19:43:08.591778Z",
     "shell.execute_reply.started": "2023-12-14T19:43:08.469780Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:43:11.109784Z",
     "iopub.status.busy": "2023-12-14T19:43:11.109335Z",
     "iopub.status.idle": "2023-12-14T19:43:11.262683Z",
     "shell.execute_reply": "2023-12-14T19:43:11.261705Z",
     "shell.execute_reply.started": "2023-12-14T19:43:11.109750Z"
    }
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(19,activation='relu'))\n",
    "model.add(Dense(1))   #regression olduğu için sadece Dense(1) yazdık. Tek bir değer tahmin edeceğiz\n",
    "model.compile(loss='mse', optimizer='adam')  #mes mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:43:35.584728Z",
     "iopub.status.busy": "2023-12-14T19:43:35.584269Z",
     "iopub.status.idle": "2023-12-14T19:44:54.922251Z",
     "shell.execute_reply": "2023-12-14T19:44:54.921131Z",
     "shell.execute_reply.started": "2023-12-14T19:43:35.584698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "96/96 [==============================] - 2s 5ms/step - loss: 15818123.0000 - val_loss: 10638333.0000\n",
      "Epoch 2/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 10158653.0000 - val_loss: 8372109.0000\n",
      "Epoch 3/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 4730298.5000 - val_loss: 2189361.7500\n",
      "Epoch 4/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 2212230.5000 - val_loss: 2104573.7500\n",
      "Epoch 5/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 2115743.0000 - val_loss: 1981571.7500\n",
      "Epoch 6/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 2032359.2500 - val_loss: 2002296.7500\n",
      "Epoch 7/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 1935666.1250 - val_loss: 1843527.3750\n",
      "Epoch 8/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1851010.8750 - val_loss: 1741542.0000\n",
      "Epoch 9/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 1756599.6250 - val_loss: 1731767.1250\n",
      "Epoch 10/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 1666465.3750 - val_loss: 1572553.6250\n",
      "Epoch 11/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 1570032.2500 - val_loss: 1465954.6250\n",
      "Epoch 12/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 1468961.2500 - val_loss: 1554628.8750\n",
      "Epoch 13/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1405359.6250 - val_loss: 1304170.6250\n",
      "Epoch 14/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1317475.3750 - val_loss: 1424507.8750\n",
      "Epoch 15/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1252755.2500 - val_loss: 1177473.6250\n",
      "Epoch 16/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 1207413.0000 - val_loss: 1125564.8750\n",
      "Epoch 17/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 1120275.0000 - val_loss: 1069363.6250\n",
      "Epoch 18/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1042137.0625 - val_loss: 993785.6250\n",
      "Epoch 19/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 994352.2500 - val_loss: 934823.3750\n",
      "Epoch 20/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 905540.6875 - val_loss: 863978.5625\n",
      "Epoch 21/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 884093.3750 - val_loss: 898687.7500\n",
      "Epoch 22/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 805996.4375 - val_loss: 746138.5625\n",
      "Epoch 23/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 757168.0000 - val_loss: 754984.9375\n",
      "Epoch 24/300\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 702183.6875 - val_loss: 646314.6875\n",
      "Epoch 25/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 698421.1875 - val_loss: 645000.3750\n",
      "Epoch 26/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 608840.6875 - val_loss: 563528.0000\n",
      "Epoch 27/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 584348.8750 - val_loss: 577639.6250\n",
      "Epoch 28/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 548036.3125 - val_loss: 552998.5625\n",
      "Epoch 29/300\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 522462.7500 - val_loss: 509464.2812\n",
      "Epoch 30/300\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 531798.6875 - val_loss: 504604.8125\n",
      "Epoch 31/300\n",
      "96/96 [==============================] - 1s 5ms/step - loss: 514337.5938 - val_loss: 564486.3125\n",
      "Epoch 32/300\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 505721.7188 - val_loss: 472241.0625\n",
      "Epoch 33/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 474430.7812 - val_loss: 490124.8125\n",
      "Epoch 34/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 488454.0312 - val_loss: 473020.8750\n",
      "Epoch 35/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 492320.7188 - val_loss: 508852.7500\n",
      "Epoch 36/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 486152.1562 - val_loss: 461798.3125\n",
      "Epoch 37/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 480932.7500 - val_loss: 540946.1875\n",
      "Epoch 38/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 482193.2812 - val_loss: 458179.5938\n",
      "Epoch 39/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 478221.1875 - val_loss: 453578.1562\n",
      "Epoch 40/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 473715.4375 - val_loss: 451827.0938\n",
      "Epoch 41/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 457764.2188 - val_loss: 465573.3438\n",
      "Epoch 42/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 458120.0312 - val_loss: 467716.6875\n",
      "Epoch 43/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 499544.2188 - val_loss: 473329.7812\n",
      "Epoch 44/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 502369.4062 - val_loss: 458499.3438\n",
      "Epoch 45/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 479496.9062 - val_loss: 510539.0625\n",
      "Epoch 46/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 464435.2812 - val_loss: 471830.6875\n",
      "Epoch 47/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 471626.3125 - val_loss: 449668.8125\n",
      "Epoch 48/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 468271.8125 - val_loss: 560678.3750\n",
      "Epoch 49/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 478818.3750 - val_loss: 474383.0625\n",
      "Epoch 50/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 475826.0312 - val_loss: 519668.5312\n",
      "Epoch 51/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 470812.6875 - val_loss: 470276.9375\n",
      "Epoch 52/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 464943.2188 - val_loss: 461196.2500\n",
      "Epoch 53/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 473066.4375 - val_loss: 449503.8125\n",
      "Epoch 54/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 487028.2812 - val_loss: 449505.6250\n",
      "Epoch 55/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 484561.8125 - val_loss: 482934.1562\n",
      "Epoch 56/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 458817.3125 - val_loss: 480913.0000\n",
      "Epoch 57/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 480213.1250 - val_loss: 473285.0938\n",
      "Epoch 58/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 482409.5312 - val_loss: 454605.0938\n",
      "Epoch 59/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 480918.0312 - val_loss: 448960.3750\n",
      "Epoch 60/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 489798.6250 - val_loss: 450530.1875\n",
      "Epoch 61/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459258.4062 - val_loss: 522409.0938\n",
      "Epoch 62/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 463884.2188 - val_loss: 469076.4688\n",
      "Epoch 63/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 475638.7500 - val_loss: 448597.5000\n",
      "Epoch 64/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 472417.0625 - val_loss: 483019.5625\n",
      "Epoch 65/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 494950.7188 - val_loss: 447960.2812\n",
      "Epoch 66/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 460909.5000 - val_loss: 448639.5312\n",
      "Epoch 67/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 467649.4375 - val_loss: 508624.7812\n",
      "Epoch 68/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 466459.9062 - val_loss: 445772.1875\n",
      "Epoch 69/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 464287.7188 - val_loss: 449694.0938\n",
      "Epoch 70/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 487096.1562 - val_loss: 513616.4062\n",
      "Epoch 71/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 479775.0938 - val_loss: 455140.6250\n",
      "Epoch 72/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 482523.1875 - val_loss: 479026.4062\n",
      "Epoch 73/300\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 484701.2812 - val_loss: 472355.4062\n",
      "Epoch 74/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 465392.5000 - val_loss: 519900.8750\n",
      "Epoch 75/300\n",
      "96/96 [==============================] - 0s 5ms/step - loss: 465690.3125 - val_loss: 448266.9688\n",
      "Epoch 76/300\n",
      "96/96 [==============================] - 0s 5ms/step - loss: 463373.4375 - val_loss: 506336.0938\n",
      "Epoch 77/300\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 481878.5312 - val_loss: 482325.9062\n",
      "Epoch 78/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 493834.0312 - val_loss: 452640.3438\n",
      "Epoch 79/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 475231.0312 - val_loss: 507199.4062\n",
      "Epoch 80/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 458705.1250 - val_loss: 450489.2500\n",
      "Epoch 81/300\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 466337.4688 - val_loss: 471119.2812\n",
      "Epoch 82/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 464305.6250 - val_loss: 445678.0312\n",
      "Epoch 83/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 475728.9062 - val_loss: 459066.7812\n",
      "Epoch 84/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 470315.7188 - val_loss: 462677.6875\n",
      "Epoch 85/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 477757.2812 - val_loss: 464159.9062\n",
      "Epoch 86/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 500402.3750 - val_loss: 529005.3750\n",
      "Epoch 87/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 486331.0000 - val_loss: 570010.3125\n",
      "Epoch 88/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 460024.1875 - val_loss: 460909.0000\n",
      "Epoch 89/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 458770.6562 - val_loss: 445371.7188\n",
      "Epoch 90/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459095.4688 - val_loss: 460300.8438\n",
      "Epoch 91/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 455994.6250 - val_loss: 444184.9375\n",
      "Epoch 92/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 466021.4688 - val_loss: 502706.0625\n",
      "Epoch 93/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 503987.1562 - val_loss: 473154.9688\n",
      "Epoch 94/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 509487.8125 - val_loss: 506582.5938\n",
      "Epoch 95/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 467841.7812 - val_loss: 464598.4688\n",
      "Epoch 96/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 467344.9375 - val_loss: 452690.7188\n",
      "Epoch 97/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 491511.4062 - val_loss: 523249.2188\n",
      "Epoch 98/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 456497.7812 - val_loss: 464348.2500\n",
      "Epoch 99/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 469469.2188 - val_loss: 454470.9375\n",
      "Epoch 100/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 467017.3125 - val_loss: 479250.1562\n",
      "Epoch 101/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 480409.4688 - val_loss: 481661.4375\n",
      "Epoch 102/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 464047.5312 - val_loss: 520558.9375\n",
      "Epoch 103/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459123.1250 - val_loss: 457249.1562\n",
      "Epoch 104/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 465476.7500 - val_loss: 443137.7500\n",
      "Epoch 105/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459648.0312 - val_loss: 456537.1875\n",
      "Epoch 106/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 474460.1875 - val_loss: 493066.0938\n",
      "Epoch 107/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 462688.0000 - val_loss: 449410.3125\n",
      "Epoch 108/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 467870.7188 - val_loss: 461946.4375\n",
      "Epoch 109/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 475839.7188 - val_loss: 444031.1250\n",
      "Epoch 110/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450209.7812 - val_loss: 444000.4062\n",
      "Epoch 111/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459592.9375 - val_loss: 510488.5000\n",
      "Epoch 112/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 502203.3438 - val_loss: 489478.9062\n",
      "Epoch 113/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 466568.5938 - val_loss: 527241.6875\n",
      "Epoch 114/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 481022.7500 - val_loss: 477079.0938\n",
      "Epoch 115/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459248.7188 - val_loss: 451679.8438\n",
      "Epoch 116/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451328.0625 - val_loss: 594193.5625\n",
      "Epoch 117/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 483178.5625 - val_loss: 447245.1875\n",
      "Epoch 118/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450396.0625 - val_loss: 455594.6250\n",
      "Epoch 119/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 464219.1562 - val_loss: 466236.0312\n",
      "Epoch 120/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 470277.7500 - val_loss: 666797.1250\n",
      "Epoch 121/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 483418.5000 - val_loss: 557886.5000\n",
      "Epoch 122/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 475858.8125 - val_loss: 444120.1250\n",
      "Epoch 123/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 463273.1875 - val_loss: 490865.7188\n",
      "Epoch 124/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 468325.9062 - val_loss: 478396.2812\n",
      "Epoch 125/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 510724.8750 - val_loss: 446122.2812\n",
      "Epoch 126/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452412.3438 - val_loss: 451832.7188\n",
      "Epoch 127/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449707.8125 - val_loss: 445634.5625\n",
      "Epoch 128/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 465494.1562 - val_loss: 445132.1250\n",
      "Epoch 129/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 453742.9688 - val_loss: 444033.8125\n",
      "Epoch 130/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 466136.9688 - val_loss: 464530.0000\n",
      "Epoch 131/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 455920.2812 - val_loss: 538878.3125\n",
      "Epoch 132/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 476530.3125 - val_loss: 447729.9375\n",
      "Epoch 133/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450160.2812 - val_loss: 447751.7500\n",
      "Epoch 134/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 478525.0938 - val_loss: 448886.9688\n",
      "Epoch 135/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 474724.9375 - val_loss: 548001.6250\n",
      "Epoch 136/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 483850.0000 - val_loss: 442750.6250\n",
      "Epoch 137/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449655.0625 - val_loss: 454729.9062\n",
      "Epoch 138/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 464450.3125 - val_loss: 443306.8125\n",
      "Epoch 139/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459882.2500 - val_loss: 528341.8750\n",
      "Epoch 140/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 480274.9688 - val_loss: 450801.9062\n",
      "Epoch 141/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 464511.5000 - val_loss: 454383.4375\n",
      "Epoch 142/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 489144.0625 - val_loss: 507255.0000\n",
      "Epoch 143/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 495258.0938 - val_loss: 445501.5312\n",
      "Epoch 144/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 477594.4062 - val_loss: 615056.6875\n",
      "Epoch 145/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 483155.5000 - val_loss: 456655.3750\n",
      "Epoch 146/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 467649.3125 - val_loss: 463257.2812\n",
      "Epoch 147/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444749.7500 - val_loss: 457617.7812\n",
      "Epoch 148/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452598.5938 - val_loss: 446668.4375\n",
      "Epoch 149/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step - loss: 467261.9375 - val_loss: 443344.4375\n",
      "Epoch 150/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 473470.3438 - val_loss: 486319.1875\n",
      "Epoch 151/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 488804.3750 - val_loss: 440889.5000\n",
      "Epoch 152/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448130.1875 - val_loss: 444752.8125\n",
      "Epoch 153/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 466374.2188 - val_loss: 475389.4375\n",
      "Epoch 154/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450223.6875 - val_loss: 441616.8750\n",
      "Epoch 155/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459006.3125 - val_loss: 459716.6562\n",
      "Epoch 156/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450485.2812 - val_loss: 463339.2500\n",
      "Epoch 157/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 456612.3125 - val_loss: 448953.4375\n",
      "Epoch 158/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 461984.0000 - val_loss: 448128.2500\n",
      "Epoch 159/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445473.5000 - val_loss: 440412.8750\n",
      "Epoch 160/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 452893.1250 - val_loss: 443295.2812\n",
      "Epoch 161/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 462731.4062 - val_loss: 568779.7500\n",
      "Epoch 162/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 465708.5312 - val_loss: 439117.0625\n",
      "Epoch 163/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 455920.6250 - val_loss: 438806.1562\n",
      "Epoch 164/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 454608.0000 - val_loss: 473326.6250\n",
      "Epoch 165/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 456917.5625 - val_loss: 446133.7500\n",
      "Epoch 166/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 458978.5625 - val_loss: 469643.4688\n",
      "Epoch 167/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 464290.9062 - val_loss: 490104.3750\n",
      "Epoch 168/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 464401.3750 - val_loss: 449303.6562\n",
      "Epoch 169/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446933.5625 - val_loss: 443386.7188\n",
      "Epoch 170/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 462596.1250 - val_loss: 549422.1875\n",
      "Epoch 171/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 467842.9688 - val_loss: 440481.3125\n",
      "Epoch 172/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 464535.3125 - val_loss: 448881.7188\n",
      "Epoch 173/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 454255.9375 - val_loss: 441292.3438\n",
      "Epoch 174/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 447486.4375 - val_loss: 455845.0312\n",
      "Epoch 175/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 461573.4062 - val_loss: 443135.1562\n",
      "Epoch 176/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452610.5000 - val_loss: 443890.5312\n",
      "Epoch 177/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 461241.9375 - val_loss: 453684.0625\n",
      "Epoch 178/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 480579.3750 - val_loss: 439010.9375\n",
      "Epoch 179/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 448406.4688 - val_loss: 456350.7500\n",
      "Epoch 180/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 471120.5000 - val_loss: 439265.7812\n",
      "Epoch 181/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 486467.1562 - val_loss: 445968.6250\n",
      "Epoch 182/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 463833.6562 - val_loss: 471926.3438\n",
      "Epoch 183/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 454263.5625 - val_loss: 444810.9688\n",
      "Epoch 184/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 457322.3125 - val_loss: 437968.4375\n",
      "Epoch 185/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 457771.6250 - val_loss: 443897.2188\n",
      "Epoch 186/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 442890.2188 - val_loss: 443056.9688\n",
      "Epoch 187/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 455187.8125 - val_loss: 487717.0000\n",
      "Epoch 188/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 473713.4062 - val_loss: 448779.3438\n",
      "Epoch 189/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 451855.3125 - val_loss: 439630.7812\n",
      "Epoch 190/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 451077.7500 - val_loss: 437580.8125\n",
      "Epoch 191/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 447593.2500 - val_loss: 452493.7188\n",
      "Epoch 192/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 478106.1562 - val_loss: 461423.0000\n",
      "Epoch 193/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 458364.0000 - val_loss: 439901.8750\n",
      "Epoch 194/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 461152.8750 - val_loss: 442922.7500\n",
      "Epoch 195/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 448626.1250 - val_loss: 439352.5938\n",
      "Epoch 196/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 450117.5625 - val_loss: 446932.2188\n",
      "Epoch 197/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 492486.6875 - val_loss: 477814.5938\n",
      "Epoch 198/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 451348.8125 - val_loss: 518641.2812\n",
      "Epoch 199/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 468462.1250 - val_loss: 447663.9688\n",
      "Epoch 200/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 460432.2500 - val_loss: 438712.6250\n",
      "Epoch 201/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 470766.0625 - val_loss: 454272.7500\n",
      "Epoch 202/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 453876.0938 - val_loss: 445886.7812\n",
      "Epoch 203/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 455219.4062 - val_loss: 473898.1875\n",
      "Epoch 204/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 454357.9375 - val_loss: 439367.9375\n",
      "Epoch 205/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 474055.4375 - val_loss: 505103.0000\n",
      "Epoch 206/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 463509.0938 - val_loss: 504364.7812\n",
      "Epoch 207/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 456887.9062 - val_loss: 457831.8125\n",
      "Epoch 208/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 455549.9375 - val_loss: 453093.1250\n",
      "Epoch 209/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 438900.2500 - val_loss: 454596.3438\n",
      "Epoch 210/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 490963.6562 - val_loss: 448186.0938\n",
      "Epoch 211/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 453598.8125 - val_loss: 464692.1562\n",
      "Epoch 212/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 454425.3750 - val_loss: 509154.2500\n",
      "Epoch 213/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 454270.7500 - val_loss: 436328.1875\n",
      "Epoch 214/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438315.6250 - val_loss: 441971.7812\n",
      "Epoch 215/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449830.5625 - val_loss: 450618.0938\n",
      "Epoch 216/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459063.7500 - val_loss: 438030.0625\n",
      "Epoch 217/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 459514.6562 - val_loss: 465205.6562\n",
      "Epoch 218/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 457675.0938 - val_loss: 438160.7812\n",
      "Epoch 219/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 459914.6875 - val_loss: 474733.6562\n",
      "Epoch 220/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450372.6250 - val_loss: 457342.8125\n",
      "Epoch 221/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450437.4688 - val_loss: 542591.0625\n",
      "Epoch 222/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 482585.5000 - val_loss: 445983.5938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459330.5938 - val_loss: 447272.3438\n",
      "Epoch 224/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 468555.4375 - val_loss: 437089.6250\n",
      "Epoch 225/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459847.1562 - val_loss: 470195.2812\n",
      "Epoch 226/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 462790.0938 - val_loss: 449768.9688\n",
      "Epoch 227/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 456107.7812 - val_loss: 454426.1250\n",
      "Epoch 228/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446946.5625 - val_loss: 457067.7812\n",
      "Epoch 229/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450075.9062 - val_loss: 476631.0625\n",
      "Epoch 230/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 453004.1250 - val_loss: 467881.7188\n",
      "Epoch 231/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 447805.9375 - val_loss: 435982.4375\n",
      "Epoch 232/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 455991.0625 - val_loss: 451277.6562\n",
      "Epoch 233/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 490352.3750 - val_loss: 461710.3438\n",
      "Epoch 234/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 454479.0625 - val_loss: 456679.5938\n",
      "Epoch 235/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451940.1250 - val_loss: 437774.0938\n",
      "Epoch 236/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 447571.8125 - val_loss: 458205.4375\n",
      "Epoch 237/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 460689.4062 - val_loss: 456026.3750\n",
      "Epoch 238/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451973.0938 - val_loss: 481645.3750\n",
      "Epoch 239/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 461048.0625 - val_loss: 545065.3125\n",
      "Epoch 240/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 457034.1875 - val_loss: 448505.3125\n",
      "Epoch 241/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451115.4375 - val_loss: 447046.3125\n",
      "Epoch 242/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 467036.8750 - val_loss: 547515.9375\n",
      "Epoch 243/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 475313.0312 - val_loss: 442172.7812\n",
      "Epoch 244/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445820.3750 - val_loss: 439522.5312\n",
      "Epoch 245/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443996.2188 - val_loss: 462849.3438\n",
      "Epoch 246/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 481755.6562 - val_loss: 437456.0312\n",
      "Epoch 247/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449702.5312 - val_loss: 446363.1250\n",
      "Epoch 248/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 460610.3438 - val_loss: 449061.2812\n",
      "Epoch 249/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444637.2812 - val_loss: 469108.6562\n",
      "Epoch 250/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 453983.8438 - val_loss: 500970.8125\n",
      "Epoch 251/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 455702.0625 - val_loss: 457042.0938\n",
      "Epoch 252/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452904.2188 - val_loss: 443175.2188\n",
      "Epoch 253/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 465099.6875 - val_loss: 434823.0000\n",
      "Epoch 254/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 454040.2188 - val_loss: 479954.1250\n",
      "Epoch 255/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 465507.1562 - val_loss: 445794.3125\n",
      "Epoch 256/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451553.6562 - val_loss: 441200.0625\n",
      "Epoch 257/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448245.7500 - val_loss: 503365.7812\n",
      "Epoch 258/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 457843.9062 - val_loss: 462279.0625\n",
      "Epoch 259/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446465.1250 - val_loss: 504764.2188\n",
      "Epoch 260/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451783.1875 - val_loss: 441107.0938\n",
      "Epoch 261/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 465722.1250 - val_loss: 446610.3750\n",
      "Epoch 262/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444465.9062 - val_loss: 447549.4062\n",
      "Epoch 263/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 477462.1250 - val_loss: 626571.4375\n",
      "Epoch 264/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 461021.4062 - val_loss: 526725.3125\n",
      "Epoch 265/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449806.7188 - val_loss: 433860.6875\n",
      "Epoch 266/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 453110.0938 - val_loss: 447697.4688\n",
      "Epoch 267/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 490027.6875 - val_loss: 496483.3750\n",
      "Epoch 268/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444977.8125 - val_loss: 450027.0625\n",
      "Epoch 269/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 460771.9688 - val_loss: 439734.0000\n",
      "Epoch 270/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 447172.4375 - val_loss: 434805.2500\n",
      "Epoch 271/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 449668.0312 - val_loss: 434540.1250\n",
      "Epoch 272/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438796.6875 - val_loss: 434696.4062\n",
      "Epoch 273/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 462576.2500 - val_loss: 437058.9375\n",
      "Epoch 274/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451086.7812 - val_loss: 498527.0938\n",
      "Epoch 275/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 456808.1875 - val_loss: 439417.0625\n",
      "Epoch 276/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 453949.2500 - val_loss: 456110.5938\n",
      "Epoch 277/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 470371.4062 - val_loss: 471508.3125\n",
      "Epoch 278/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449379.0938 - val_loss: 493936.9375\n",
      "Epoch 279/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450376.9062 - val_loss: 435940.2500\n",
      "Epoch 280/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 453063.8750 - val_loss: 435845.1875\n",
      "Epoch 281/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 441747.0000 - val_loss: 434134.2812\n",
      "Epoch 282/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441527.5312 - val_loss: 515118.7500\n",
      "Epoch 283/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444927.1250 - val_loss: 435251.1562\n",
      "Epoch 284/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 461393.7812 - val_loss: 448656.6875\n",
      "Epoch 285/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 477557.9375 - val_loss: 450238.0000\n",
      "Epoch 286/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438996.2500 - val_loss: 433200.7188\n",
      "Epoch 287/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450188.7812 - val_loss: 457369.0312\n",
      "Epoch 288/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 456546.2188 - val_loss: 437142.8125\n",
      "Epoch 289/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 460085.5938 - val_loss: 435534.4375\n",
      "Epoch 290/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 446810.5312 - val_loss: 440037.7188\n",
      "Epoch 291/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 460114.4062 - val_loss: 443462.9062\n",
      "Epoch 292/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 454666.3438 - val_loss: 437255.8125\n",
      "Epoch 293/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451181.4375 - val_loss: 440321.0000\n",
      "Epoch 294/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 463718.4688 - val_loss: 434273.4062\n",
      "Epoch 295/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440390.4375 - val_loss: 434722.9062\n",
      "Epoch 296/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450921.3750 - val_loss: 433958.0938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439038.0938 - val_loss: 436626.1562\n",
      "Epoch 298/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443748.2500 - val_loss: 437077.1875\n",
      "Epoch 299/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 463067.9375 - val_loss: 466887.3750\n",
      "Epoch 300/300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 453849.9375 - val_loss: 435205.6875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f51025dc50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=128,epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:45:03.634340Z",
     "iopub.status.busy": "2023-12-14T19:45:03.633784Z",
     "iopub.status.idle": "2023-12-14T19:45:03.665492Z",
     "shell.execute_reply": "2023-12-14T19:45:03.664250Z",
     "shell.execute_reply.started": "2023-12-14T19:45:03.634294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 19)                342       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 19)                380       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 19)                380       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 19)                380       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 19)                380       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1882 (7.35 KB)\n",
      "Trainable params: 1882 (7.35 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:45:13.837178Z",
     "iopub.status.busy": "2023-12-14T19:45:13.836674Z",
     "iopub.status.idle": "2023-12-14T19:45:13.844902Z",
     "shell.execute_reply": "2023-12-14T19:45:13.843534Z",
     "shell.execute_reply.started": "2023-12-14T19:45:13.837121Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_df=pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T19:45:22.234562Z",
     "iopub.status.busy": "2023-12-14T19:45:22.233204Z",
     "iopub.status.idle": "2023-12-14T19:45:22.699041Z",
     "shell.execute_reply": "2023-12-14T19:45:22.696416Z",
     "shell.execute_reply.started": "2023-12-14T19:45:22.234517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "tahmin=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752433717072955"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,tahmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659.7011568634249"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,tahmin)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1300\n",
      "96/96 [==============================] - 0s 5ms/step - loss: 450731.2812 - val_loss: 469745.2812\n",
      "Epoch 2/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 445349.1562 - val_loss: 477592.4375\n",
      "Epoch 3/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 445030.2812 - val_loss: 465934.6875\n",
      "Epoch 4/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 437675.6875 - val_loss: 438600.9688\n",
      "Epoch 5/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 464905.5312 - val_loss: 681235.6875\n",
      "Epoch 6/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 465631.8750 - val_loss: 438947.2812\n",
      "Epoch 7/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437226.3438 - val_loss: 457859.0625\n",
      "Epoch 8/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444832.1875 - val_loss: 448314.8125\n",
      "Epoch 9/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 448684.2188 - val_loss: 485352.5938\n",
      "Epoch 10/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 455354.8750 - val_loss: 437177.2188\n",
      "Epoch 11/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 433618.3438 - val_loss: 447385.2812\n",
      "Epoch 12/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448826.0000 - val_loss: 460691.7188\n",
      "Epoch 13/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 439940.3750 - val_loss: 443041.5312\n",
      "Epoch 14/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 442741.0625 - val_loss: 432029.2500\n",
      "Epoch 15/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 440149.6562 - val_loss: 463380.5938\n",
      "Epoch 16/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 460744.8750 - val_loss: 484772.5000\n",
      "Epoch 17/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 469001.2812 - val_loss: 559347.8750\n",
      "Epoch 18/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 457686.4375 - val_loss: 434570.7188\n",
      "Epoch 19/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444758.0938 - val_loss: 434353.1250\n",
      "Epoch 20/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439041.0625 - val_loss: 431607.8438\n",
      "Epoch 21/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 436387.5000 - val_loss: 453055.4062\n",
      "Epoch 22/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 473322.7500 - val_loss: 435838.1250\n",
      "Epoch 23/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 450545.9375 - val_loss: 488243.5000\n",
      "Epoch 24/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 464460.2188 - val_loss: 478186.4375\n",
      "Epoch 25/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 456853.9062 - val_loss: 507476.5625\n",
      "Epoch 26/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 443993.7188 - val_loss: 438100.1875\n",
      "Epoch 27/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 451621.6875 - val_loss: 495750.0000\n",
      "Epoch 28/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 459969.7188 - val_loss: 450600.9375\n",
      "Epoch 29/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434404.2812 - val_loss: 472279.4062\n",
      "Epoch 30/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 450775.2188 - val_loss: 434161.8125\n",
      "Epoch 31/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 445491.8125 - val_loss: 431749.3125\n",
      "Epoch 32/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 448209.5938 - val_loss: 439695.3125\n",
      "Epoch 33/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 448642.6562 - val_loss: 432485.3438\n",
      "Epoch 34/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 453147.9688 - val_loss: 441529.4688\n",
      "Epoch 35/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441790.6250 - val_loss: 460475.5938\n",
      "Epoch 36/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 457157.2812 - val_loss: 436349.8750\n",
      "Epoch 37/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 468783.6250 - val_loss: 469614.6562\n",
      "Epoch 38/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438868.6875 - val_loss: 431140.8438\n",
      "Epoch 39/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 436336.5312 - val_loss: 472425.9688\n",
      "Epoch 40/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 442108.8125 - val_loss: 519379.0000\n",
      "Epoch 41/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 452356.5000 - val_loss: 430103.4062\n",
      "Epoch 42/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 441707.9375 - val_loss: 445040.9688\n",
      "Epoch 43/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 463277.5000 - val_loss: 431233.8438\n",
      "Epoch 44/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444368.5625 - val_loss: 433132.4375\n",
      "Epoch 45/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 441537.6562 - val_loss: 519150.3750\n",
      "Epoch 46/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 451750.7812 - val_loss: 531033.3125\n",
      "Epoch 47/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 465917.5000 - val_loss: 451178.7500\n",
      "Epoch 48/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 447479.7188 - val_loss: 447953.2812\n",
      "Epoch 49/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 459599.3125 - val_loss: 506678.2188\n",
      "Epoch 50/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451375.5312 - val_loss: 434961.1250\n",
      "Epoch 51/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 451657.4375 - val_loss: 440309.9062\n",
      "Epoch 52/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 447277.0312 - val_loss: 434454.3438\n",
      "Epoch 53/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 438487.4062 - val_loss: 450462.2188\n",
      "Epoch 54/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 455004.1250 - val_loss: 431545.4688\n",
      "Epoch 55/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 444283.6250 - val_loss: 447745.8438\n",
      "Epoch 56/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 437988.7812 - val_loss: 440873.3125\n",
      "Epoch 57/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 467513.0625 - val_loss: 452478.3125\n",
      "Epoch 58/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 443177.1250 - val_loss: 447611.9688\n",
      "Epoch 59/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 479030.9062 - val_loss: 544149.7500\n",
      "Epoch 60/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 463181.8750 - val_loss: 440333.4062\n",
      "Epoch 61/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 443385.0938 - val_loss: 432850.6562\n",
      "Epoch 62/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 455504.0312 - val_loss: 431442.5938\n",
      "Epoch 63/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 437719.0938 - val_loss: 455645.0000\n",
      "Epoch 64/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452078.9688 - val_loss: 469648.7500\n",
      "Epoch 65/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 449308.2812 - val_loss: 430887.6875\n",
      "Epoch 66/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433004.7188 - val_loss: 630370.1250\n",
      "Epoch 67/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 482789.4688 - val_loss: 431101.5000\n",
      "Epoch 68/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 437913.8750 - val_loss: 436725.0312\n",
      "Epoch 69/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 448763.1250 - val_loss: 442558.9062\n",
      "Epoch 70/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 447276.0938 - val_loss: 429972.0000\n",
      "Epoch 71/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 442018.3750 - val_loss: 430215.6250\n",
      "Epoch 72/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450818.0312 - val_loss: 439692.8125\n",
      "Epoch 73/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 441686.9688 - val_loss: 438694.3125\n",
      "Epoch 74/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 441069.4688 - val_loss: 429709.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444091.4062 - val_loss: 437059.7188\n",
      "Epoch 76/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438107.3125 - val_loss: 547478.6250\n",
      "Epoch 77/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 461418.4375 - val_loss: 490691.5625\n",
      "Epoch 78/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452646.8750 - val_loss: 459078.1875\n",
      "Epoch 79/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437299.0625 - val_loss: 443605.9688\n",
      "Epoch 80/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 463149.1875 - val_loss: 746121.5625\n",
      "Epoch 81/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 479725.0312 - val_loss: 444515.7500\n",
      "Epoch 82/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443583.8125 - val_loss: 431196.9375\n",
      "Epoch 83/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 468978.7812 - val_loss: 438617.3125\n",
      "Epoch 84/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 436384.1250 - val_loss: 439417.2188\n",
      "Epoch 85/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448517.1875 - val_loss: 433081.9688\n",
      "Epoch 86/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438059.5625 - val_loss: 429784.7500\n",
      "Epoch 87/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440078.5000 - val_loss: 432781.6875\n",
      "Epoch 88/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445707.6250 - val_loss: 432850.4062\n",
      "Epoch 89/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 444100.2812 - val_loss: 429469.9688\n",
      "Epoch 90/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434890.9062 - val_loss: 436737.7500\n",
      "Epoch 91/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438718.5312 - val_loss: 429820.2188\n",
      "Epoch 92/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 447763.1250 - val_loss: 428707.9688\n",
      "Epoch 93/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 446729.4062 - val_loss: 434607.7500\n",
      "Epoch 94/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435631.2500 - val_loss: 438998.8125\n",
      "Epoch 95/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450708.9062 - val_loss: 444120.6250\n",
      "Epoch 96/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444396.5625 - val_loss: 464338.6562\n",
      "Epoch 97/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 453637.2812 - val_loss: 434980.5312\n",
      "Epoch 98/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 435026.9062 - val_loss: 444733.5625\n",
      "Epoch 99/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 437515.9375 - val_loss: 454782.6562\n",
      "Epoch 100/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438529.0625 - val_loss: 428814.0625\n",
      "Epoch 101/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448671.2812 - val_loss: 428304.5312\n",
      "Epoch 102/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 441684.6562 - val_loss: 523518.1562\n",
      "Epoch 103/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441913.5625 - val_loss: 429006.0312\n",
      "Epoch 104/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445548.6562 - val_loss: 428587.0000\n",
      "Epoch 105/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 439969.9062 - val_loss: 522292.8750\n",
      "Epoch 106/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 453202.8438 - val_loss: 432271.9375\n",
      "Epoch 107/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435308.9688 - val_loss: 438633.0938\n",
      "Epoch 108/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450347.2188 - val_loss: 455596.7812\n",
      "Epoch 109/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 462456.9375 - val_loss: 451815.5000\n",
      "Epoch 110/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450690.8438 - val_loss: 434496.7500\n",
      "Epoch 111/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441090.0000 - val_loss: 449258.5312\n",
      "Epoch 112/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437593.8438 - val_loss: 428331.4062\n",
      "Epoch 113/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441451.7812 - val_loss: 430584.0625\n",
      "Epoch 114/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438764.9062 - val_loss: 429591.3438\n",
      "Epoch 115/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 458579.6562 - val_loss: 450214.1250\n",
      "Epoch 116/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 437119.9062 - val_loss: 526167.5000\n",
      "Epoch 117/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 444302.5938 - val_loss: 434050.5625\n",
      "Epoch 118/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 452201.3750 - val_loss: 431123.2500\n",
      "Epoch 119/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 431592.9688 - val_loss: 489886.5000\n",
      "Epoch 120/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 441071.9062 - val_loss: 515611.5000\n",
      "Epoch 121/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451055.8125 - val_loss: 436142.3438\n",
      "Epoch 122/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451070.9688 - val_loss: 443922.7188\n",
      "Epoch 123/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432681.3438 - val_loss: 435896.6875\n",
      "Epoch 124/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446795.6562 - val_loss: 427744.6562\n",
      "Epoch 125/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 454500.0312 - val_loss: 447005.1875\n",
      "Epoch 126/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439230.3125 - val_loss: 432716.3750\n",
      "Epoch 127/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440474.7500 - val_loss: 507378.0625\n",
      "Epoch 128/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440120.6875 - val_loss: 440778.0938\n",
      "Epoch 129/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439979.7188 - val_loss: 466120.9688\n",
      "Epoch 130/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 455478.9062 - val_loss: 442759.3750\n",
      "Epoch 131/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446768.6250 - val_loss: 457428.3125\n",
      "Epoch 132/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 457707.0625 - val_loss: 436405.7500\n",
      "Epoch 133/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443063.4062 - val_loss: 431639.1875\n",
      "Epoch 134/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446657.4062 - val_loss: 431288.0938\n",
      "Epoch 135/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434235.2812 - val_loss: 448877.1562\n",
      "Epoch 136/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442606.0938 - val_loss: 440207.3750\n",
      "Epoch 137/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452844.1875 - val_loss: 430481.0000\n",
      "Epoch 138/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 459492.0312 - val_loss: 430148.3438\n",
      "Epoch 139/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440176.6250 - val_loss: 433339.5625\n",
      "Epoch 140/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441054.6875 - val_loss: 495965.8125\n",
      "Epoch 141/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452174.5625 - val_loss: 427182.0000\n",
      "Epoch 142/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438432.2500 - val_loss: 456411.0000\n",
      "Epoch 143/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446423.3125 - val_loss: 446929.6562\n",
      "Epoch 144/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445167.5000 - val_loss: 494891.5312\n",
      "Epoch 145/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 463595.6250 - val_loss: 465150.0000\n",
      "Epoch 146/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446418.0312 - val_loss: 441556.7500\n",
      "Epoch 147/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445200.7188 - val_loss: 426999.5938\n",
      "Epoch 148/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431277.8438 - val_loss: 448762.5938\n",
      "Epoch 149/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 462897.7188 - val_loss: 427973.2188\n",
      "Epoch 150/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440005.3750 - val_loss: 466314.3750\n",
      "Epoch 151/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443297.1562 - val_loss: 445491.5312\n",
      "Epoch 152/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439645.7812 - val_loss: 429257.5000\n",
      "Epoch 153/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448003.5625 - val_loss: 455356.2812\n",
      "Epoch 154/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 445320.8125 - val_loss: 454180.7812\n",
      "Epoch 155/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 449394.9688 - val_loss: 468938.5625\n",
      "Epoch 156/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 466667.9375 - val_loss: 481602.0938\n",
      "Epoch 157/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439207.0312 - val_loss: 428020.7500\n",
      "Epoch 158/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 447430.0312 - val_loss: 435402.9375\n",
      "Epoch 159/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442900.7188 - val_loss: 442568.3750\n",
      "Epoch 160/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438320.1250 - val_loss: 427696.5000\n",
      "Epoch 161/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433552.2188 - val_loss: 436427.0938\n",
      "Epoch 162/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444699.0312 - val_loss: 496902.7500\n",
      "Epoch 163/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 448726.5312 - val_loss: 428091.0938\n",
      "Epoch 164/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 438396.0000 - val_loss: 427499.8125\n",
      "Epoch 165/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 449142.3750 - val_loss: 431841.3125\n",
      "Epoch 166/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 432036.1250 - val_loss: 488432.1875\n",
      "Epoch 167/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 436272.5625 - val_loss: 465738.9375\n",
      "Epoch 168/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 432676.2812 - val_loss: 465247.7188\n",
      "Epoch 169/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 442006.5000 - val_loss: 462504.5312\n",
      "Epoch 170/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 448046.2500 - val_loss: 433849.4688\n",
      "Epoch 171/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441622.6562 - val_loss: 442585.9062\n",
      "Epoch 172/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 434135.9062 - val_loss: 426677.9688\n",
      "Epoch 173/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 435604.7500 - val_loss: 433454.4688\n",
      "Epoch 174/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 452027.1250 - val_loss: 437670.8750\n",
      "Epoch 175/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442313.6562 - val_loss: 463185.0000\n",
      "Epoch 176/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 443709.5938 - val_loss: 430092.6562\n",
      "Epoch 177/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452406.5938 - val_loss: 436206.2188\n",
      "Epoch 178/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 434817.4375 - val_loss: 434625.8438\n",
      "Epoch 179/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439226.7188 - val_loss: 428439.0938\n",
      "Epoch 180/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440146.1875 - val_loss: 429904.4375\n",
      "Epoch 181/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439587.0938 - val_loss: 427796.6875\n",
      "Epoch 182/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438154.7188 - val_loss: 429640.2188\n",
      "Epoch 183/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432744.0625 - val_loss: 433906.1562\n",
      "Epoch 184/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449717.0938 - val_loss: 429472.7812\n",
      "Epoch 185/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442900.2188 - val_loss: 464039.8750\n",
      "Epoch 186/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435121.4375 - val_loss: 427528.9688\n",
      "Epoch 187/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 452368.0938 - val_loss: 427877.9375\n",
      "Epoch 188/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 434780.1875 - val_loss: 427813.0938\n",
      "Epoch 189/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434781.1875 - val_loss: 427733.5000\n",
      "Epoch 190/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443647.0312 - val_loss: 426883.0938\n",
      "Epoch 191/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439807.7500 - val_loss: 442883.0938\n",
      "Epoch 192/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438007.7188 - val_loss: 435259.4688\n",
      "Epoch 193/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438194.4375 - val_loss: 445083.8438\n",
      "Epoch 194/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439983.1875 - val_loss: 438932.5000\n",
      "Epoch 195/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434304.8125 - val_loss: 428669.6250\n",
      "Epoch 196/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445913.7812 - val_loss: 427665.0312\n",
      "Epoch 197/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440050.7812 - val_loss: 433311.2812\n",
      "Epoch 198/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445336.7188 - val_loss: 464535.3125\n",
      "Epoch 199/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435840.9375 - val_loss: 449697.9062\n",
      "Epoch 200/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 446429.5938 - val_loss: 452437.0625\n",
      "Epoch 201/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 453775.7188 - val_loss: 517196.2500\n",
      "Epoch 202/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446711.2188 - val_loss: 442040.1875\n",
      "Epoch 203/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437922.5000 - val_loss: 426196.3125\n",
      "Epoch 204/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 445199.1875 - val_loss: 437821.4062\n",
      "Epoch 205/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439549.7500 - val_loss: 427674.5938\n",
      "Epoch 206/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442537.2812 - val_loss: 436936.2188\n",
      "Epoch 207/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 441523.1250 - val_loss: 426852.9062\n",
      "Epoch 208/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 447146.8438 - val_loss: 446872.1875\n",
      "Epoch 209/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437184.1562 - val_loss: 428324.5000\n",
      "Epoch 210/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 439235.2500 - val_loss: 432668.4062\n",
      "Epoch 211/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 444162.0625 - val_loss: 468761.3750\n",
      "Epoch 212/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 443371.4375 - val_loss: 433765.8125\n",
      "Epoch 213/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 439946.8125 - val_loss: 554218.0000\n",
      "Epoch 214/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 444562.4688 - val_loss: 425706.1875\n",
      "Epoch 215/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437805.2188 - val_loss: 451024.6250\n",
      "Epoch 216/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450318.9375 - val_loss: 439961.3125\n",
      "Epoch 217/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444304.5938 - val_loss: 426859.6562\n",
      "Epoch 218/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 460320.3125 - val_loss: 436957.9375\n",
      "Epoch 219/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431672.0938 - val_loss: 432444.0312\n",
      "Epoch 220/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448455.3125 - val_loss: 426434.1875\n",
      "Epoch 221/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step - loss: 440175.5312 - val_loss: 424923.4062\n",
      "Epoch 222/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 447658.0312 - val_loss: 500831.7812\n",
      "Epoch 223/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437886.5000 - val_loss: 426675.3125\n",
      "Epoch 224/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 439005.5000 - val_loss: 468223.6250\n",
      "Epoch 225/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 436734.6875 - val_loss: 433149.2812\n",
      "Epoch 226/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431406.9062 - val_loss: 425799.1250\n",
      "Epoch 227/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441409.3438 - val_loss: 425601.1875\n",
      "Epoch 228/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431892.4062 - val_loss: 436300.9375\n",
      "Epoch 229/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 441770.2188 - val_loss: 431633.9062\n",
      "Epoch 230/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448108.8750 - val_loss: 591957.4375\n",
      "Epoch 231/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 443110.3125 - val_loss: 440434.9375\n",
      "Epoch 232/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 438692.3750 - val_loss: 424438.5938\n",
      "Epoch 233/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 439192.7188 - val_loss: 449656.1875\n",
      "Epoch 234/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 447589.7188 - val_loss: 447856.1875\n",
      "Epoch 235/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 433677.9375 - val_loss: 439853.7500\n",
      "Epoch 236/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 435913.7812 - val_loss: 540335.3125\n",
      "Epoch 237/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 442321.3750 - val_loss: 429436.7812\n",
      "Epoch 238/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 431471.5938 - val_loss: 424534.6875\n",
      "Epoch 239/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430214.2188 - val_loss: 430603.8750\n",
      "Epoch 240/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 458362.6875 - val_loss: 449427.0312\n",
      "Epoch 241/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442367.6562 - val_loss: 448513.8750\n",
      "Epoch 242/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 455501.8750 - val_loss: 451586.5625\n",
      "Epoch 243/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445799.6250 - val_loss: 445644.3125\n",
      "Epoch 244/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 426970.9062 - val_loss: 424947.7812\n",
      "Epoch 245/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 439611.6250 - val_loss: 431226.8438\n",
      "Epoch 246/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432757.4688 - val_loss: 432044.9688\n",
      "Epoch 247/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441387.1875 - val_loss: 436769.8750\n",
      "Epoch 248/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434081.0312 - val_loss: 453562.0000\n",
      "Epoch 249/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 467676.3438 - val_loss: 446141.8125\n",
      "Epoch 250/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451105.6562 - val_loss: 508422.1875\n",
      "Epoch 251/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 433931.4062 - val_loss: 428592.9375\n",
      "Epoch 252/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 446853.1250 - val_loss: 450827.6250\n",
      "Epoch 253/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 442713.0312 - val_loss: 484528.0938\n",
      "Epoch 254/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 435179.0312 - val_loss: 427751.6875\n",
      "Epoch 255/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 434388.0312 - val_loss: 558342.7500\n",
      "Epoch 256/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 445405.8750 - val_loss: 458049.0938\n",
      "Epoch 257/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 438821.8438 - val_loss: 462824.4375\n",
      "Epoch 258/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432598.1250 - val_loss: 441104.1250\n",
      "Epoch 259/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 453589.0625 - val_loss: 425890.1562\n",
      "Epoch 260/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433834.0000 - val_loss: 429081.4375\n",
      "Epoch 261/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432526.8438 - val_loss: 426706.7188\n",
      "Epoch 262/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431615.9688 - val_loss: 429979.3750\n",
      "Epoch 263/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432400.2500 - val_loss: 495640.2812\n",
      "Epoch 264/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444390.3438 - val_loss: 425850.0625\n",
      "Epoch 265/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433554.4062 - val_loss: 425914.5938\n",
      "Epoch 266/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430271.6250 - val_loss: 428932.2188\n",
      "Epoch 267/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433863.5938 - val_loss: 424014.9375\n",
      "Epoch 268/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439630.8438 - val_loss: 478115.7500\n",
      "Epoch 269/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448061.6250 - val_loss: 442613.4375\n",
      "Epoch 270/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435662.9062 - val_loss: 444214.5938\n",
      "Epoch 271/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427319.0625 - val_loss: 439911.4375\n",
      "Epoch 272/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433357.9688 - val_loss: 452661.3750\n",
      "Epoch 273/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 446437.0000 - val_loss: 427232.3438\n",
      "Epoch 274/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429718.9062 - val_loss: 424988.8438\n",
      "Epoch 275/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444729.0312 - val_loss: 495236.2812\n",
      "Epoch 276/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450154.8438 - val_loss: 480163.5938\n",
      "Epoch 277/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439106.5000 - val_loss: 425235.3438\n",
      "Epoch 278/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433027.4375 - val_loss: 448237.5312\n",
      "Epoch 279/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429538.2812 - val_loss: 457302.6562\n",
      "Epoch 280/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431761.9688 - val_loss: 426112.7812\n",
      "Epoch 281/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434084.7500 - val_loss: 444884.2188\n",
      "Epoch 282/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440551.6875 - val_loss: 460346.9688\n",
      "Epoch 283/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436065.7188 - val_loss: 426891.1250\n",
      "Epoch 284/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431717.3750 - val_loss: 453031.1875\n",
      "Epoch 285/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429135.2500 - val_loss: 452452.0000\n",
      "Epoch 286/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435993.8125 - val_loss: 478022.4375\n",
      "Epoch 287/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438647.5312 - val_loss: 460829.2188\n",
      "Epoch 288/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430154.9375 - val_loss: 435333.3438\n",
      "Epoch 289/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436068.9375 - val_loss: 431322.0312\n",
      "Epoch 290/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438609.0938 - val_loss: 442941.7500\n",
      "Epoch 291/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441352.3438 - val_loss: 424658.4062\n",
      "Epoch 292/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437303.4688 - val_loss: 428673.5000\n",
      "Epoch 293/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434336.5938 - val_loss: 429045.2500\n",
      "Epoch 294/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434897.2188 - val_loss: 437061.6562\n",
      "Epoch 295/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445982.3438 - val_loss: 424942.2812\n",
      "Epoch 296/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428553.3438 - val_loss: 447070.2500\n",
      "Epoch 297/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 438218.2188 - val_loss: 450404.9688\n",
      "Epoch 298/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441703.2188 - val_loss: 452969.4688\n",
      "Epoch 299/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431813.8750 - val_loss: 426842.8750\n",
      "Epoch 300/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434253.5312 - val_loss: 470111.2188\n",
      "Epoch 301/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444261.9375 - val_loss: 444116.5000\n",
      "Epoch 302/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445993.7812 - val_loss: 558496.2500\n",
      "Epoch 303/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 446000.5938 - val_loss: 428952.7812\n",
      "Epoch 304/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451031.4062 - val_loss: 487365.2188\n",
      "Epoch 305/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 439540.0938 - val_loss: 437471.9375\n",
      "Epoch 306/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433087.7500 - val_loss: 430953.1875\n",
      "Epoch 307/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437518.0312 - val_loss: 437690.9688\n",
      "Epoch 308/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428158.7500 - val_loss: 428420.5625\n",
      "Epoch 309/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430989.6875 - val_loss: 432443.1875\n",
      "Epoch 310/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436073.8438 - val_loss: 473931.6250\n",
      "Epoch 311/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435627.0625 - val_loss: 427631.8438\n",
      "Epoch 312/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432135.7812 - val_loss: 429378.6250\n",
      "Epoch 313/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441006.0938 - val_loss: 432721.9062\n",
      "Epoch 314/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 432920.5938 - val_loss: 437531.1562\n",
      "Epoch 315/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 445045.0312 - val_loss: 474982.4688\n",
      "Epoch 316/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 436013.7812 - val_loss: 431762.6562\n",
      "Epoch 317/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443939.7812 - val_loss: 464588.6875\n",
      "Epoch 318/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 450175.4688 - val_loss: 458652.2500\n",
      "Epoch 319/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437223.2188 - val_loss: 467429.4062\n",
      "Epoch 320/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434192.5000 - val_loss: 431745.1875\n",
      "Epoch 321/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436788.0000 - val_loss: 428960.1875\n",
      "Epoch 322/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424692.6250 - val_loss: 426053.1562\n",
      "Epoch 323/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445281.1875 - val_loss: 424683.0312\n",
      "Epoch 324/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435719.3125 - val_loss: 467585.3125\n",
      "Epoch 325/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 433259.4062 - val_loss: 427570.7188\n",
      "Epoch 326/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431570.0312 - val_loss: 422296.2812\n",
      "Epoch 327/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449625.3438 - val_loss: 427591.0312\n",
      "Epoch 328/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433608.8750 - val_loss: 424480.7812\n",
      "Epoch 329/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430954.1250 - val_loss: 433252.2812\n",
      "Epoch 330/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439395.2812 - val_loss: 434558.1562\n",
      "Epoch 331/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432051.5938 - val_loss: 431039.0312\n",
      "Epoch 332/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442007.6250 - val_loss: 431439.6250\n",
      "Epoch 333/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429996.4062 - val_loss: 428898.0312\n",
      "Epoch 334/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428830.5000 - val_loss: 426889.1250\n",
      "Epoch 335/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448137.6250 - val_loss: 430553.9375\n",
      "Epoch 336/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451141.4062 - val_loss: 524989.4375\n",
      "Epoch 337/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436138.0938 - val_loss: 471262.0938\n",
      "Epoch 338/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434259.2500 - val_loss: 432336.5625\n",
      "Epoch 339/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438249.5312 - val_loss: 446106.0625\n",
      "Epoch 340/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427416.9375 - val_loss: 423570.0312\n",
      "Epoch 341/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433836.5000 - val_loss: 440180.0625\n",
      "Epoch 342/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436973.3125 - val_loss: 424606.8125\n",
      "Epoch 343/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441703.2188 - val_loss: 438903.3438\n",
      "Epoch 344/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428394.0625 - val_loss: 442482.5938\n",
      "Epoch 345/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443558.0312 - val_loss: 478009.1875\n",
      "Epoch 346/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435452.3438 - val_loss: 453135.8750\n",
      "Epoch 347/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437414.1562 - val_loss: 421342.4375\n",
      "Epoch 348/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442420.0625 - val_loss: 424732.0938\n",
      "Epoch 349/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436121.5625 - val_loss: 483469.0938\n",
      "Epoch 350/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433693.1875 - val_loss: 466019.0625\n",
      "Epoch 351/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434901.1875 - val_loss: 427324.9062\n",
      "Epoch 352/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434855.6562 - val_loss: 513742.5312\n",
      "Epoch 353/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432611.2812 - val_loss: 422446.9688\n",
      "Epoch 354/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434362.6562 - val_loss: 427929.5625\n",
      "Epoch 355/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 436202.0625 - val_loss: 470844.4375\n",
      "Epoch 356/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 430539.7500 - val_loss: 421093.8438\n",
      "Epoch 357/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436222.0312 - val_loss: 447274.5000\n",
      "Epoch 358/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 429381.5625 - val_loss: 454180.1250\n",
      "Epoch 359/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 426608.6562 - val_loss: 422808.2812\n",
      "Epoch 360/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 430436.8125 - val_loss: 425334.3438\n",
      "Epoch 361/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 436864.9375 - val_loss: 426743.5000\n",
      "Epoch 362/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 451048.6875 - val_loss: 458175.4062\n",
      "Epoch 363/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 438700.4375 - val_loss: 447857.0938\n",
      "Epoch 364/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 440471.2500 - val_loss: 457219.3438\n",
      "Epoch 365/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 452964.8125 - val_loss: 425210.1875\n",
      "Epoch 366/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 463386.4062 - val_loss: 437540.0938\n",
      "Epoch 367/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 3ms/step - loss: 435574.1875 - val_loss: 441518.5000\n",
      "Epoch 368/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435791.8750 - val_loss: 441899.1562\n",
      "Epoch 369/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436544.5312 - val_loss: 423089.7188\n",
      "Epoch 370/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431957.1250 - val_loss: 429783.1562\n",
      "Epoch 371/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444725.1875 - val_loss: 438529.9688\n",
      "Epoch 372/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 437895.6250 - val_loss: 423854.2188\n",
      "Epoch 373/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 427685.9688 - val_loss: 438884.3438\n",
      "Epoch 374/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430789.9062 - val_loss: 425311.4062\n",
      "Epoch 375/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431163.7812 - val_loss: 420842.2188\n",
      "Epoch 376/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428488.5312 - val_loss: 421465.6875\n",
      "Epoch 377/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434330.2500 - val_loss: 420493.5625\n",
      "Epoch 378/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438905.1875 - val_loss: 461068.3438\n",
      "Epoch 379/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429498.6875 - val_loss: 421053.4375\n",
      "Epoch 380/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435845.0000 - val_loss: 422276.0938\n",
      "Epoch 381/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430882.4688 - val_loss: 423710.3125\n",
      "Epoch 382/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429444.9375 - val_loss: 433821.2500\n",
      "Epoch 383/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 429104.2188 - val_loss: 421710.2188\n",
      "Epoch 384/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433959.7188 - val_loss: 506671.9375\n",
      "Epoch 385/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 440596.0625 - val_loss: 425992.3438\n",
      "Epoch 386/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439007.5000 - val_loss: 424946.6875\n",
      "Epoch 387/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448191.3750 - val_loss: 454604.9062\n",
      "Epoch 388/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435250.5625 - val_loss: 422835.8750\n",
      "Epoch 389/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427884.0625 - val_loss: 425668.2188\n",
      "Epoch 390/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444275.3125 - val_loss: 423435.8125\n",
      "Epoch 391/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428449.9688 - val_loss: 441266.6875\n",
      "Epoch 392/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429399.8750 - val_loss: 423135.6562\n",
      "Epoch 393/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430028.5938 - val_loss: 465786.7188\n",
      "Epoch 394/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443513.7812 - val_loss: 438932.7188\n",
      "Epoch 395/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432610.6562 - val_loss: 451501.8438\n",
      "Epoch 396/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428950.6875 - val_loss: 435299.5000\n",
      "Epoch 397/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435727.8125 - val_loss: 427442.8438\n",
      "Epoch 398/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427973.2812 - val_loss: 483514.7812\n",
      "Epoch 399/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438594.6562 - val_loss: 422547.5625\n",
      "Epoch 400/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432455.7812 - val_loss: 424537.6562\n",
      "Epoch 401/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443620.5625 - val_loss: 445626.0625\n",
      "Epoch 402/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440760.8438 - val_loss: 432505.5938\n",
      "Epoch 403/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 439352.5000 - val_loss: 457625.6250\n",
      "Epoch 404/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 424755.4688 - val_loss: 442704.2812\n",
      "Epoch 405/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438963.9688 - val_loss: 450695.0000\n",
      "Epoch 406/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432364.8125 - val_loss: 426774.2188\n",
      "Epoch 407/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427240.4062 - val_loss: 434787.0000\n",
      "Epoch 408/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430933.8750 - val_loss: 420179.8750\n",
      "Epoch 409/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431495.2188 - val_loss: 423082.8125\n",
      "Epoch 410/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446400.7500 - val_loss: 471121.6250\n",
      "Epoch 411/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445075.2812 - val_loss: 432877.4062\n",
      "Epoch 412/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432938.6875 - val_loss: 434550.0938\n",
      "Epoch 413/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435836.3750 - val_loss: 449575.1875\n",
      "Epoch 414/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427634.7812 - val_loss: 422594.9375\n",
      "Epoch 415/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439832.2500 - val_loss: 441706.6250\n",
      "Epoch 416/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429175.9688 - val_loss: 437021.1250\n",
      "Epoch 417/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438228.1562 - val_loss: 432710.6562\n",
      "Epoch 418/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442899.9688 - val_loss: 422059.6875\n",
      "Epoch 419/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429816.6250 - val_loss: 421609.2812\n",
      "Epoch 420/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 423702.5938 - val_loss: 421405.5938\n",
      "Epoch 421/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424057.5938 - val_loss: 420525.7188\n",
      "Epoch 422/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443272.4375 - val_loss: 424305.3125\n",
      "Epoch 423/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432679.0938 - val_loss: 421361.1875\n",
      "Epoch 424/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438941.8438 - val_loss: 421044.5625\n",
      "Epoch 425/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427996.3438 - val_loss: 432845.1562\n",
      "Epoch 426/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435219.1875 - val_loss: 458612.0625\n",
      "Epoch 427/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438755.6562 - val_loss: 505090.7500\n",
      "Epoch 428/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 448837.5000 - val_loss: 421881.7500\n",
      "Epoch 429/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 447173.7500 - val_loss: 420303.2500\n",
      "Epoch 430/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425295.0000 - val_loss: 422471.4062\n",
      "Epoch 431/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425105.1250 - val_loss: 422878.5938\n",
      "Epoch 432/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441467.1875 - val_loss: 432955.7188\n",
      "Epoch 433/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440451.7812 - val_loss: 466728.0625\n",
      "Epoch 434/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436973.7188 - val_loss: 458676.9062\n",
      "Epoch 435/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428957.0938 - val_loss: 427929.9375\n",
      "Epoch 436/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425738.3125 - val_loss: 428047.6562\n",
      "Epoch 437/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442068.2500 - val_loss: 420527.3125\n",
      "Epoch 438/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432980.8750 - val_loss: 485241.0938\n",
      "Epoch 439/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435241.3125 - val_loss: 423569.5312\n",
      "Epoch 440/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428049.4062 - val_loss: 420141.4688\n",
      "Epoch 441/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432718.4062 - val_loss: 533874.3750\n",
      "Epoch 442/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426738.9688 - val_loss: 466575.9062\n",
      "Epoch 443/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431077.5000 - val_loss: 420176.5312\n",
      "Epoch 444/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441824.6562 - val_loss: 504519.2500\n",
      "Epoch 445/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430037.7188 - val_loss: 419334.2812\n",
      "Epoch 446/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435566.0625 - val_loss: 562249.6875\n",
      "Epoch 447/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449547.8125 - val_loss: 425217.3750\n",
      "Epoch 448/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442272.6562 - val_loss: 444487.4375\n",
      "Epoch 449/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434690.5000 - val_loss: 455444.9375\n",
      "Epoch 450/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441137.7500 - val_loss: 419471.6250\n",
      "Epoch 451/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428811.2500 - val_loss: 472320.9688\n",
      "Epoch 452/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423202.0000 - val_loss: 458614.6250\n",
      "Epoch 453/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436454.6250 - val_loss: 476448.5625\n",
      "Epoch 454/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440025.1875 - val_loss: 459739.1250\n",
      "Epoch 455/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431667.6562 - val_loss: 422163.0312\n",
      "Epoch 456/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430757.0938 - val_loss: 426001.8438\n",
      "Epoch 457/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 431058.0000 - val_loss: 419842.7188\n",
      "Epoch 458/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442871.3750 - val_loss: 420781.7500\n",
      "Epoch 459/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427435.0312 - val_loss: 455794.1875\n",
      "Epoch 460/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441816.0625 - val_loss: 511153.2812\n",
      "Epoch 461/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445999.5625 - val_loss: 421044.6250\n",
      "Epoch 462/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431423.9688 - val_loss: 419351.0312\n",
      "Epoch 463/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432100.2188 - val_loss: 418990.3750\n",
      "Epoch 464/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430911.0938 - val_loss: 439384.8750\n",
      "Epoch 465/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444520.3438 - val_loss: 436868.8125\n",
      "Epoch 466/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434087.8750 - val_loss: 420375.0312\n",
      "Epoch 467/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 421432.1875 - val_loss: 419137.0312\n",
      "Epoch 468/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428098.4062 - val_loss: 424102.4688\n",
      "Epoch 469/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422381.6250 - val_loss: 448916.0625\n",
      "Epoch 470/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438792.2500 - val_loss: 445394.4062\n",
      "Epoch 471/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438868.0625 - val_loss: 422172.8438\n",
      "Epoch 472/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429022.3750 - val_loss: 422762.9062\n",
      "Epoch 473/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428782.4062 - val_loss: 425067.6875\n",
      "Epoch 474/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427174.1562 - val_loss: 444153.4375\n",
      "Epoch 475/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426882.1250 - val_loss: 425908.2812\n",
      "Epoch 476/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 457130.5938 - val_loss: 423918.5312\n",
      "Epoch 477/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423747.9062 - val_loss: 423235.0000\n",
      "Epoch 478/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421883.7188 - val_loss: 455965.6250\n",
      "Epoch 479/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427459.7188 - val_loss: 417984.4688\n",
      "Epoch 480/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433986.5000 - val_loss: 419651.3125\n",
      "Epoch 481/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437677.5312 - val_loss: 419715.7188\n",
      "Epoch 482/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422531.2812 - val_loss: 430815.8438\n",
      "Epoch 483/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438364.7500 - val_loss: 420603.9688\n",
      "Epoch 484/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434926.9062 - val_loss: 432397.5312\n",
      "Epoch 485/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421683.8750 - val_loss: 418272.4688\n",
      "Epoch 486/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428275.6250 - val_loss: 424570.5938\n",
      "Epoch 487/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424587.7188 - val_loss: 427126.0312\n",
      "Epoch 488/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428081.8438 - val_loss: 428343.9688\n",
      "Epoch 489/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 451035.3438 - val_loss: 442278.3125\n",
      "Epoch 490/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423089.3125 - val_loss: 421142.6562\n",
      "Epoch 491/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430948.2812 - val_loss: 463519.0625\n",
      "Epoch 492/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 447578.9062 - val_loss: 559153.8125\n",
      "Epoch 493/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 434463.8750 - val_loss: 419451.9688\n",
      "Epoch 494/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427952.2500 - val_loss: 442293.3750\n",
      "Epoch 495/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426226.1250 - val_loss: 443213.0938\n",
      "Epoch 496/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436698.3125 - val_loss: 425715.7500\n",
      "Epoch 497/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428101.6875 - val_loss: 420234.8438\n",
      "Epoch 498/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435180.2812 - val_loss: 458928.6875\n",
      "Epoch 499/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428067.3438 - val_loss: 419351.8125\n",
      "Epoch 500/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431979.9062 - val_loss: 460511.4688\n",
      "Epoch 501/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428332.9062 - val_loss: 418184.6562\n",
      "Epoch 502/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442875.4688 - val_loss: 421793.8750\n",
      "Epoch 503/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424777.3750 - val_loss: 488022.1875\n",
      "Epoch 504/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443557.2188 - val_loss: 418963.1562\n",
      "Epoch 505/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428707.4375 - val_loss: 437766.5312\n",
      "Epoch 506/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428991.2500 - val_loss: 418820.4062\n",
      "Epoch 507/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435315.5312 - val_loss: 421552.0000\n",
      "Epoch 508/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431963.1875 - val_loss: 447025.6875\n",
      "Epoch 509/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 430903.3125 - val_loss: 430001.1250\n",
      "Epoch 510/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 431688.6562 - val_loss: 421199.9375\n",
      "Epoch 511/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 430697.3125 - val_loss: 424421.5312\n",
      "Epoch 512/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426052.0938 - val_loss: 435152.7812\n",
      "Epoch 513/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step - loss: 427350.4688 - val_loss: 440857.7500\n",
      "Epoch 514/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429166.0000 - val_loss: 422581.9688\n",
      "Epoch 515/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 440205.5000 - val_loss: 451571.5312\n",
      "Epoch 516/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 440479.2500 - val_loss: 430155.5938\n",
      "Epoch 517/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435228.9375 - val_loss: 418468.0000\n",
      "Epoch 518/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428114.5938 - val_loss: 424946.8438\n",
      "Epoch 519/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437206.2500 - val_loss: 446894.2188\n",
      "Epoch 520/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446246.1562 - val_loss: 460646.1250\n",
      "Epoch 521/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436223.3750 - val_loss: 444828.6250\n",
      "Epoch 522/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423246.1875 - val_loss: 421421.9062\n",
      "Epoch 523/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431824.0625 - val_loss: 471230.9062\n",
      "Epoch 524/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434037.6875 - val_loss: 484112.1562\n",
      "Epoch 525/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437408.5938 - val_loss: 419876.5000\n",
      "Epoch 526/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438232.6562 - val_loss: 419733.6875\n",
      "Epoch 527/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430857.9062 - val_loss: 441302.3438\n",
      "Epoch 528/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426058.3125 - val_loss: 419077.0312\n",
      "Epoch 529/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435344.4688 - val_loss: 442144.4375\n",
      "Epoch 530/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420746.4062 - val_loss: 419539.1562\n",
      "Epoch 531/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440293.4688 - val_loss: 431883.4375\n",
      "Epoch 532/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431130.7188 - val_loss: 472502.5000\n",
      "Epoch 533/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441253.1250 - val_loss: 418601.0938\n",
      "Epoch 534/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428102.9688 - val_loss: 421858.4375\n",
      "Epoch 535/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449643.9688 - val_loss: 420566.3125\n",
      "Epoch 536/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421706.5312 - val_loss: 418330.5000\n",
      "Epoch 537/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439389.5625 - val_loss: 429577.7500\n",
      "Epoch 538/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428823.6875 - val_loss: 451616.3438\n",
      "Epoch 539/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437889.9688 - val_loss: 429482.7500\n",
      "Epoch 540/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426252.2812 - val_loss: 528727.7500\n",
      "Epoch 541/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431992.6562 - val_loss: 445453.2188\n",
      "Epoch 542/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429679.5625 - val_loss: 482250.7500\n",
      "Epoch 543/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 461318.8750 - val_loss: 518171.7812\n",
      "Epoch 544/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441438.4062 - val_loss: 418588.0312\n",
      "Epoch 545/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436599.1562 - val_loss: 438390.2188\n",
      "Epoch 546/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424906.5000 - val_loss: 418125.3125\n",
      "Epoch 547/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425000.4688 - val_loss: 447257.0625\n",
      "Epoch 548/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 436636.0938 - val_loss: 456906.1562\n",
      "Epoch 549/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426251.7812 - val_loss: 473916.9062\n",
      "Epoch 550/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436032.9062 - val_loss: 422619.3438\n",
      "Epoch 551/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442904.1875 - val_loss: 417617.1250\n",
      "Epoch 552/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 424939.3125 - val_loss: 462727.8750\n",
      "Epoch 553/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 442726.0625 - val_loss: 421967.6562\n",
      "Epoch 554/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 422815.8125 - val_loss: 422387.4375\n",
      "Epoch 555/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430622.2188 - val_loss: 460940.3125\n",
      "Epoch 556/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422391.5938 - val_loss: 416777.2812\n",
      "Epoch 557/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433743.2812 - val_loss: 476205.6250\n",
      "Epoch 558/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424376.6562 - val_loss: 449504.2188\n",
      "Epoch 559/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432395.1562 - val_loss: 436884.0625\n",
      "Epoch 560/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444448.5312 - val_loss: 452145.6562\n",
      "Epoch 561/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436596.9375 - val_loss: 417037.6875\n",
      "Epoch 562/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 449830.1875 - val_loss: 447290.0625\n",
      "Epoch 563/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 431460.6875 - val_loss: 419547.6562\n",
      "Epoch 564/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 422843.0625 - val_loss: 423271.3750\n",
      "Epoch 565/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 426654.3125 - val_loss: 417481.5000\n",
      "Epoch 566/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429807.5312 - val_loss: 490982.3438\n",
      "Epoch 567/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427123.1562 - val_loss: 450422.5000\n",
      "Epoch 568/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434063.7188 - val_loss: 420953.7500\n",
      "Epoch 569/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426016.5312 - val_loss: 492602.4688\n",
      "Epoch 570/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 455512.2500 - val_loss: 417738.3438\n",
      "Epoch 571/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424190.7188 - val_loss: 417358.7812\n",
      "Epoch 572/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431393.4375 - val_loss: 428975.8750\n",
      "Epoch 573/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427062.4688 - val_loss: 417397.6250\n",
      "Epoch 574/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428129.6562 - val_loss: 440767.1562\n",
      "Epoch 575/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427395.7500 - val_loss: 426741.3750\n",
      "Epoch 576/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 458667.7812 - val_loss: 423950.2812\n",
      "Epoch 577/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424756.2500 - val_loss: 425221.8125\n",
      "Epoch 578/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 431845.0938 - val_loss: 419308.3750\n",
      "Epoch 579/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 418124.5938 - val_loss: 419900.7812\n",
      "Epoch 580/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422916.8125 - val_loss: 416952.6875\n",
      "Epoch 581/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430471.6875 - val_loss: 420402.7188\n",
      "Epoch 582/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422332.8438 - val_loss: 431410.4375\n",
      "Epoch 583/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 433152.5312 - val_loss: 419075.7188\n",
      "Epoch 584/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 427588.2188 - val_loss: 471480.3750\n",
      "Epoch 585/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423105.0625 - val_loss: 455336.9062\n",
      "Epoch 586/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422897.4688 - val_loss: 421800.9062\n",
      "Epoch 587/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429946.0000 - val_loss: 425052.9688\n",
      "Epoch 588/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425635.9688 - val_loss: 457791.8750\n",
      "Epoch 589/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423648.8750 - val_loss: 425303.9375\n",
      "Epoch 590/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423817.7500 - val_loss: 416265.4062\n",
      "Epoch 591/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 449954.7812 - val_loss: 467726.0625\n",
      "Epoch 592/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 428943.7812 - val_loss: 445372.6562\n",
      "Epoch 593/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430167.5625 - val_loss: 442526.9688\n",
      "Epoch 594/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440633.1250 - val_loss: 417628.9688\n",
      "Epoch 595/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420076.8750 - val_loss: 433634.9375\n",
      "Epoch 596/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426807.3125 - val_loss: 421916.6250\n",
      "Epoch 597/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427345.4688 - val_loss: 421434.0625\n",
      "Epoch 598/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425237.5000 - val_loss: 449123.5312\n",
      "Epoch 599/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429323.6562 - val_loss: 425106.7500\n",
      "Epoch 600/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425191.0000 - val_loss: 431536.9688\n",
      "Epoch 601/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 435114.2812 - val_loss: 447374.8125\n",
      "Epoch 602/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 431453.7500 - val_loss: 457703.1250\n",
      "Epoch 603/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 432920.7500 - val_loss: 423119.2812\n",
      "Epoch 604/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421231.6562 - val_loss: 417967.9375\n",
      "Epoch 605/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426629.1250 - val_loss: 415663.8750\n",
      "Epoch 606/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422031.3438 - val_loss: 419141.2812\n",
      "Epoch 607/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434895.8750 - val_loss: 420264.4375\n",
      "Epoch 608/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429208.0938 - val_loss: 423372.0938\n",
      "Epoch 609/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432247.3125 - val_loss: 424166.0625\n",
      "Epoch 610/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430615.8125 - val_loss: 433848.0938\n",
      "Epoch 611/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421186.6562 - val_loss: 422803.4688\n",
      "Epoch 612/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426754.2812 - val_loss: 421763.4688\n",
      "Epoch 613/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425419.6250 - val_loss: 441034.8125\n",
      "Epoch 614/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421924.7500 - val_loss: 418058.6875\n",
      "Epoch 615/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445285.1562 - val_loss: 490253.4062\n",
      "Epoch 616/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431208.7500 - val_loss: 435239.7188\n",
      "Epoch 617/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420660.9688 - val_loss: 424160.9375\n",
      "Epoch 618/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422503.8125 - val_loss: 417215.7812\n",
      "Epoch 619/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428311.5312 - val_loss: 427618.7812\n",
      "Epoch 620/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443797.1562 - val_loss: 461697.9375\n",
      "Epoch 621/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 435534.2500 - val_loss: 417603.0938\n",
      "Epoch 622/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 432969.3125 - val_loss: 416110.2500\n",
      "Epoch 623/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 430240.7188 - val_loss: 439913.0312\n",
      "Epoch 624/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 424684.0000 - val_loss: 424274.0312\n",
      "Epoch 625/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429921.1875 - val_loss: 429640.1562\n",
      "Epoch 626/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424330.2500 - val_loss: 433756.2188\n",
      "Epoch 627/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431736.2812 - val_loss: 415428.6562\n",
      "Epoch 628/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421506.8125 - val_loss: 443815.3750\n",
      "Epoch 629/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427342.5000 - val_loss: 415784.7188\n",
      "Epoch 630/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 426299.8438 - val_loss: 439900.3438\n",
      "Epoch 631/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 428786.7500 - val_loss: 415773.4688\n",
      "Epoch 632/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437191.0625 - val_loss: 452426.3438\n",
      "Epoch 633/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430803.0625 - val_loss: 465030.9375\n",
      "Epoch 634/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422398.6562 - val_loss: 424311.5938\n",
      "Epoch 635/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429211.3125 - val_loss: 429715.5625\n",
      "Epoch 636/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428952.6562 - val_loss: 422359.2188\n",
      "Epoch 637/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426729.2188 - val_loss: 427324.5625\n",
      "Epoch 638/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431925.9062 - val_loss: 417145.1875\n",
      "Epoch 639/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427294.6562 - val_loss: 481086.0000\n",
      "Epoch 640/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431583.0000 - val_loss: 416820.7500\n",
      "Epoch 641/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429782.1250 - val_loss: 443424.4688\n",
      "Epoch 642/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445311.4375 - val_loss: 431647.4062\n",
      "Epoch 643/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428894.2188 - val_loss: 423790.8438\n",
      "Epoch 644/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424549.3438 - val_loss: 443308.8750\n",
      "Epoch 645/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424267.9062 - val_loss: 450635.3125\n",
      "Epoch 646/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429001.7812 - val_loss: 417310.1250\n",
      "Epoch 647/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423715.8438 - val_loss: 414980.5312\n",
      "Epoch 648/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 419560.0000 - val_loss: 456995.4688\n",
      "Epoch 649/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426048.4375 - val_loss: 428434.5000\n",
      "Epoch 650/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429285.4375 - val_loss: 438464.0000\n",
      "Epoch 651/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423895.9062 - val_loss: 420982.0312\n",
      "Epoch 652/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426170.4062 - val_loss: 434163.3125\n",
      "Epoch 653/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435413.5625 - val_loss: 460034.2188\n",
      "Epoch 654/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437879.7500 - val_loss: 419487.7812\n",
      "Epoch 655/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428253.9062 - val_loss: 477931.3125\n",
      "Epoch 656/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427076.6562 - val_loss: 449320.3125\n",
      "Epoch 657/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430732.3125 - val_loss: 442130.2812\n",
      "Epoch 658/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417450.4688 - val_loss: 437521.0625\n",
      "Epoch 659/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step - loss: 429287.9375 - val_loss: 464777.8438\n",
      "Epoch 660/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427116.7500 - val_loss: 416573.3125\n",
      "Epoch 661/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419990.1875 - val_loss: 420868.2188\n",
      "Epoch 662/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421225.5938 - val_loss: 439691.4688\n",
      "Epoch 663/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427773.0625 - val_loss: 468781.7188\n",
      "Epoch 664/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433756.5625 - val_loss: 416081.0312\n",
      "Epoch 665/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426203.9375 - val_loss: 428040.3750\n",
      "Epoch 666/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427425.5312 - val_loss: 472948.8750\n",
      "Epoch 667/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426561.4375 - val_loss: 452081.1250\n",
      "Epoch 668/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420821.6875 - val_loss: 416492.5000\n",
      "Epoch 669/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433738.6875 - val_loss: 421260.0000\n",
      "Epoch 670/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434186.8750 - val_loss: 469738.6250\n",
      "Epoch 671/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428929.0000 - val_loss: 417704.8438\n",
      "Epoch 672/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424938.9062 - val_loss: 425077.0625\n",
      "Epoch 673/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428002.5625 - val_loss: 419325.6562\n",
      "Epoch 674/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422436.2812 - val_loss: 480606.7188\n",
      "Epoch 675/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428661.5312 - val_loss: 415050.9688\n",
      "Epoch 676/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417510.3750 - val_loss: 428434.3750\n",
      "Epoch 677/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428019.7812 - val_loss: 418204.7500\n",
      "Epoch 678/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433284.1875 - val_loss: 419635.6875\n",
      "Epoch 679/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433516.9375 - val_loss: 459016.5938\n",
      "Epoch 680/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436215.5312 - val_loss: 417411.8125\n",
      "Epoch 681/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422015.5938 - val_loss: 427737.0625\n",
      "Epoch 682/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424329.9375 - val_loss: 424107.2500\n",
      "Epoch 683/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441024.1875 - val_loss: 416529.5938\n",
      "Epoch 684/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422387.2188 - val_loss: 421702.4375\n",
      "Epoch 685/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 434560.2812 - val_loss: 460747.3125\n",
      "Epoch 686/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 432214.8750 - val_loss: 415687.0000\n",
      "Epoch 687/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426120.1875 - val_loss: 415089.1875\n",
      "Epoch 688/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423424.9062 - val_loss: 417208.0000\n",
      "Epoch 689/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439008.3438 - val_loss: 421831.7812\n",
      "Epoch 690/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419656.7812 - val_loss: 431890.3438\n",
      "Epoch 691/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 455937.4375 - val_loss: 425130.2188\n",
      "Epoch 692/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427811.1562 - val_loss: 421506.1250\n",
      "Epoch 693/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446320.1562 - val_loss: 525139.9375\n",
      "Epoch 694/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433758.9688 - val_loss: 417641.4062\n",
      "Epoch 695/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426940.5625 - val_loss: 418823.7500\n",
      "Epoch 696/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 445035.0000 - val_loss: 421861.9688\n",
      "Epoch 697/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427352.7188 - val_loss: 424219.9688\n",
      "Epoch 698/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423461.9688 - val_loss: 417498.0000\n",
      "Epoch 699/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422361.4375 - val_loss: 418997.1562\n",
      "Epoch 700/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427855.0938 - val_loss: 459376.4375\n",
      "Epoch 701/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420619.7500 - val_loss: 418839.6875\n",
      "Epoch 702/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422594.0625 - val_loss: 461280.1562\n",
      "Epoch 703/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428419.6250 - val_loss: 415732.6875\n",
      "Epoch 704/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420006.4062 - val_loss: 428181.3750\n",
      "Epoch 705/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434683.7812 - val_loss: 509151.5625\n",
      "Epoch 706/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426606.9688 - val_loss: 415895.4062\n",
      "Epoch 707/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423532.2500 - val_loss: 415409.1875\n",
      "Epoch 708/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422596.8438 - val_loss: 413926.9375\n",
      "Epoch 709/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428249.3750 - val_loss: 449223.8750\n",
      "Epoch 710/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425060.5938 - val_loss: 420164.3438\n",
      "Epoch 711/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423749.8750 - val_loss: 436061.4688\n",
      "Epoch 712/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426241.8125 - val_loss: 414073.6875\n",
      "Epoch 713/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424267.0312 - val_loss: 505119.9375\n",
      "Epoch 714/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439702.5938 - val_loss: 431746.3750\n",
      "Epoch 715/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431571.5938 - val_loss: 413899.5938\n",
      "Epoch 716/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418445.3750 - val_loss: 420779.6875\n",
      "Epoch 717/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425616.4062 - val_loss: 421156.1562\n",
      "Epoch 718/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 434832.7500 - val_loss: 416587.3438\n",
      "Epoch 719/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 422431.0938 - val_loss: 415655.3438\n",
      "Epoch 720/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422990.4062 - val_loss: 507151.0000\n",
      "Epoch 721/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420687.2188 - val_loss: 414881.7500\n",
      "Epoch 722/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428409.3750 - val_loss: 479877.0625\n",
      "Epoch 723/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424697.9062 - val_loss: 413834.0312\n",
      "Epoch 724/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418796.6562 - val_loss: 418501.4375\n",
      "Epoch 725/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 422997.2812 - val_loss: 500762.4688\n",
      "Epoch 726/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419501.8750 - val_loss: 414331.2812\n",
      "Epoch 727/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438337.8125 - val_loss: 445100.8438\n",
      "Epoch 728/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425595.7812 - val_loss: 437518.1562\n",
      "Epoch 729/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430692.8438 - val_loss: 415777.9375\n",
      "Epoch 730/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429197.1562 - val_loss: 458311.9375\n",
      "Epoch 731/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422720.3750 - val_loss: 423884.8125\n",
      "Epoch 732/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428013.7188 - val_loss: 448794.1875\n",
      "Epoch 733/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424253.4375 - val_loss: 427401.0000\n",
      "Epoch 734/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434462.7812 - val_loss: 429783.2188\n",
      "Epoch 735/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426599.9688 - val_loss: 470483.0312\n",
      "Epoch 736/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425270.6875 - val_loss: 415946.7188\n",
      "Epoch 737/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416355.4062 - val_loss: 424723.7188\n",
      "Epoch 738/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415931.2812 - val_loss: 420433.5312\n",
      "Epoch 739/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425116.2812 - val_loss: 427709.9375\n",
      "Epoch 740/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418132.2812 - val_loss: 414901.2500\n",
      "Epoch 741/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417904.5625 - val_loss: 418910.8125\n",
      "Epoch 742/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421643.7812 - val_loss: 431965.8750\n",
      "Epoch 743/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421007.9062 - val_loss: 494465.0938\n",
      "Epoch 744/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428365.4062 - val_loss: 422685.0938\n",
      "Epoch 745/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428845.4375 - val_loss: 433374.0625\n",
      "Epoch 746/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418926.7188 - val_loss: 424118.1250\n",
      "Epoch 747/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 419199.2812 - val_loss: 413154.6250\n",
      "Epoch 748/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 425720.2812 - val_loss: 412953.2812\n",
      "Epoch 749/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427345.0625 - val_loss: 455194.8750\n",
      "Epoch 750/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433486.5625 - val_loss: 441945.2812\n",
      "Epoch 751/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429954.1250 - val_loss: 461778.6562\n",
      "Epoch 752/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420175.6250 - val_loss: 419820.9062\n",
      "Epoch 753/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 422482.0938 - val_loss: 472773.8125\n",
      "Epoch 754/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433605.3125 - val_loss: 435524.7812\n",
      "Epoch 755/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436153.5938 - val_loss: 454125.0000\n",
      "Epoch 756/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418658.3438 - val_loss: 416841.1250\n",
      "Epoch 757/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417125.1250 - val_loss: 440104.3438\n",
      "Epoch 758/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417685.0938 - val_loss: 415584.0312\n",
      "Epoch 759/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418840.8750 - val_loss: 434739.6875\n",
      "Epoch 760/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413540.2500 - val_loss: 449726.7812\n",
      "Epoch 761/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419283.3438 - val_loss: 412802.8750\n",
      "Epoch 762/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420507.8438 - val_loss: 427412.2188\n",
      "Epoch 763/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416515.8750 - val_loss: 436210.4688\n",
      "Epoch 764/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425635.8125 - val_loss: 463691.6250\n",
      "Epoch 765/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430067.1562 - val_loss: 416323.6875\n",
      "Epoch 766/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425024.3750 - val_loss: 413559.9688\n",
      "Epoch 767/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422506.2812 - val_loss: 416158.4375\n",
      "Epoch 768/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 423193.3438 - val_loss: 422304.3125\n",
      "Epoch 769/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424009.8750 - val_loss: 531717.6250\n",
      "Epoch 770/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424944.8438 - val_loss: 412289.4375\n",
      "Epoch 771/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420711.9062 - val_loss: 413231.0000\n",
      "Epoch 772/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419737.7188 - val_loss: 414740.5000\n",
      "Epoch 773/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421497.6562 - val_loss: 424610.2500\n",
      "Epoch 774/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430104.9062 - val_loss: 442676.0625\n",
      "Epoch 775/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419406.3750 - val_loss: 414072.1250\n",
      "Epoch 776/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436283.4688 - val_loss: 412778.6562\n",
      "Epoch 777/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437391.8750 - val_loss: 415533.0000\n",
      "Epoch 778/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432768.0625 - val_loss: 417492.2188\n",
      "Epoch 779/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 417448.3125 - val_loss: 413413.0938\n",
      "Epoch 780/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 419712.0000 - val_loss: 452001.6562\n",
      "Epoch 781/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421587.0625 - val_loss: 439655.3750\n",
      "Epoch 782/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417710.4062 - val_loss: 416250.9688\n",
      "Epoch 783/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431477.4375 - val_loss: 483759.2188\n",
      "Epoch 784/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439794.3438 - val_loss: 447923.2188\n",
      "Epoch 785/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421289.0625 - val_loss: 413223.3438\n",
      "Epoch 786/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 417157.2188 - val_loss: 413135.4062\n",
      "Epoch 787/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 421131.5000 - val_loss: 430648.1250\n",
      "Epoch 788/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416630.3750 - val_loss: 421388.8125\n",
      "Epoch 789/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418721.6562 - val_loss: 439710.2188\n",
      "Epoch 790/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434254.7500 - val_loss: 470803.3750\n",
      "Epoch 791/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431583.7188 - val_loss: 411445.0000\n",
      "Epoch 792/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419737.4375 - val_loss: 460342.8438\n",
      "Epoch 793/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426004.0938 - val_loss: 416305.9375\n",
      "Epoch 794/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423245.4688 - val_loss: 440167.9375\n",
      "Epoch 795/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427499.0625 - val_loss: 412932.9062\n",
      "Epoch 796/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424206.5938 - val_loss: 425754.5000\n",
      "Epoch 797/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430006.4062 - val_loss: 412517.3438\n",
      "Epoch 798/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421304.2500 - val_loss: 414864.6875\n",
      "Epoch 799/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416468.6250 - val_loss: 421326.9062\n",
      "Epoch 800/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426433.7500 - val_loss: 420658.7812\n",
      "Epoch 801/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417116.3750 - val_loss: 434052.8125\n",
      "Epoch 802/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433278.4062 - val_loss: 469006.5938\n",
      "Epoch 803/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429626.2188 - val_loss: 443733.8125\n",
      "Epoch 804/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424303.3750 - val_loss: 410596.4688\n",
      "Epoch 805/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step - loss: 427697.4062 - val_loss: 416699.1562\n",
      "Epoch 806/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428612.6875 - val_loss: 416380.6562\n",
      "Epoch 807/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415224.5625 - val_loss: 419666.5000\n",
      "Epoch 808/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426107.2500 - val_loss: 421188.4375\n",
      "Epoch 809/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422333.7812 - val_loss: 430892.5312\n",
      "Epoch 810/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419715.3750 - val_loss: 413108.6562\n",
      "Epoch 811/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423462.8750 - val_loss: 412332.0625\n",
      "Epoch 812/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418742.9062 - val_loss: 413177.6250\n",
      "Epoch 813/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416949.2188 - val_loss: 410410.6562\n",
      "Epoch 814/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414278.8438 - val_loss: 448050.5625\n",
      "Epoch 815/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430388.2500 - val_loss: 427436.7500\n",
      "Epoch 816/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424840.1875 - val_loss: 533111.3125\n",
      "Epoch 817/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436511.0938 - val_loss: 413230.0000\n",
      "Epoch 818/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417803.0625 - val_loss: 418597.5938\n",
      "Epoch 819/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432794.1875 - val_loss: 426816.0938\n",
      "Epoch 820/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430584.2188 - val_loss: 428127.3438\n",
      "Epoch 821/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430938.5312 - val_loss: 429890.4375\n",
      "Epoch 822/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424135.0000 - val_loss: 414116.2500\n",
      "Epoch 823/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421284.3125 - val_loss: 414528.1562\n",
      "Epoch 824/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417894.2812 - val_loss: 418121.2500\n",
      "Epoch 825/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 416443.2500 - val_loss: 437335.2812\n",
      "Epoch 826/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428867.9062 - val_loss: 411542.9688\n",
      "Epoch 827/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426350.8750 - val_loss: 411130.1250\n",
      "Epoch 828/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423345.5625 - val_loss: 412442.1875\n",
      "Epoch 829/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421968.2812 - val_loss: 411088.0000\n",
      "Epoch 830/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 434638.9375 - val_loss: 414620.7188\n",
      "Epoch 831/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418945.4062 - val_loss: 474367.8750\n",
      "Epoch 832/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422752.5000 - val_loss: 414675.4688\n",
      "Epoch 833/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426231.5938 - val_loss: 517036.3125\n",
      "Epoch 834/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418939.8750 - val_loss: 414386.6562\n",
      "Epoch 835/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429171.9062 - val_loss: 412237.8438\n",
      "Epoch 836/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 413795.6562 - val_loss: 416216.6562\n",
      "Epoch 837/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 431189.6562 - val_loss: 468448.0625\n",
      "Epoch 838/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 444125.2812 - val_loss: 424080.7812\n",
      "Epoch 839/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417104.7188 - val_loss: 418149.8438\n",
      "Epoch 840/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426153.2812 - val_loss: 429998.2188\n",
      "Epoch 841/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418696.0000 - val_loss: 424544.1875\n",
      "Epoch 842/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427322.2188 - val_loss: 435633.6875\n",
      "Epoch 843/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423588.9375 - val_loss: 410192.8125\n",
      "Epoch 844/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416346.5938 - val_loss: 439349.8438\n",
      "Epoch 845/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425902.0938 - val_loss: 409778.0625\n",
      "Epoch 846/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418730.8125 - val_loss: 423368.9688\n",
      "Epoch 847/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418246.5312 - val_loss: 412002.9375\n",
      "Epoch 848/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416646.2812 - val_loss: 462217.5000\n",
      "Epoch 849/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436326.4375 - val_loss: 461699.9375\n",
      "Epoch 850/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417467.8438 - val_loss: 445998.5312\n",
      "Epoch 851/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420923.0312 - val_loss: 429005.7188\n",
      "Epoch 852/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421998.8125 - val_loss: 435030.3438\n",
      "Epoch 853/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417554.7188 - val_loss: 410896.2500\n",
      "Epoch 854/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415441.6250 - val_loss: 416844.4375\n",
      "Epoch 855/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426831.5625 - val_loss: 446337.0312\n",
      "Epoch 856/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423337.5938 - val_loss: 410571.3750\n",
      "Epoch 857/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421735.4688 - val_loss: 418076.3750\n",
      "Epoch 858/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 427034.2188 - val_loss: 430040.6875\n",
      "Epoch 859/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420708.4062 - val_loss: 423365.5625\n",
      "Epoch 860/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427772.5312 - val_loss: 411657.0312\n",
      "Epoch 861/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423965.3438 - val_loss: 480349.3750\n",
      "Epoch 862/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437337.9688 - val_loss: 432771.5938\n",
      "Epoch 863/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419785.6250 - val_loss: 410053.6562\n",
      "Epoch 864/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418480.1875 - val_loss: 437342.5625\n",
      "Epoch 865/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 441580.7188 - val_loss: 417802.6875\n",
      "Epoch 866/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421081.8125 - val_loss: 463407.7500\n",
      "Epoch 867/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418423.8438 - val_loss: 414301.5000\n",
      "Epoch 868/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414826.7500 - val_loss: 412398.0938\n",
      "Epoch 869/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415199.9375 - val_loss: 410590.0000\n",
      "Epoch 870/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422663.9062 - val_loss: 430672.5000\n",
      "Epoch 871/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 416983.0000 - val_loss: 409600.5312\n",
      "Epoch 872/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415743.5000 - val_loss: 417502.7188\n",
      "Epoch 873/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423522.7812 - val_loss: 420760.3125\n",
      "Epoch 874/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416138.9062 - val_loss: 415247.7812\n",
      "Epoch 875/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416584.8438 - val_loss: 486539.2500\n",
      "Epoch 876/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414257.9062 - val_loss: 410119.0000\n",
      "Epoch 877/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414788.9375 - val_loss: 412068.9688\n",
      "Epoch 878/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423950.5312 - val_loss: 422658.0625\n",
      "Epoch 879/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415741.4375 - val_loss: 423373.5938\n",
      "Epoch 880/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 413910.9688 - val_loss: 411963.3438\n",
      "Epoch 881/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419850.5000 - val_loss: 410371.0625\n",
      "Epoch 882/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421791.0625 - val_loss: 420939.2188\n",
      "Epoch 883/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424267.1562 - val_loss: 412784.2500\n",
      "Epoch 884/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419300.5938 - val_loss: 410645.0938\n",
      "Epoch 885/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419940.7500 - val_loss: 412060.5000\n",
      "Epoch 886/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418553.5625 - val_loss: 410463.2188\n",
      "Epoch 887/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440686.0625 - val_loss: 419740.6250\n",
      "Epoch 888/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416988.4062 - val_loss: 425822.4688\n",
      "Epoch 889/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424690.0000 - val_loss: 412186.4688\n",
      "Epoch 890/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425791.8438 - val_loss: 434959.1562\n",
      "Epoch 891/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423558.3438 - val_loss: 411623.1875\n",
      "Epoch 892/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425713.6250 - val_loss: 442201.9375\n",
      "Epoch 893/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418348.5000 - val_loss: 479726.6250\n",
      "Epoch 894/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418886.1562 - val_loss: 442349.6562\n",
      "Epoch 895/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421959.5938 - val_loss: 411023.6875\n",
      "Epoch 896/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416619.6562 - val_loss: 435150.0625\n",
      "Epoch 897/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417968.2188 - val_loss: 408548.2500\n",
      "Epoch 898/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 434492.8750 - val_loss: 411069.5625\n",
      "Epoch 899/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427893.0312 - val_loss: 412102.1562\n",
      "Epoch 900/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410456.0625 - val_loss: 485196.0000\n",
      "Epoch 901/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 439561.4688 - val_loss: 410673.9688\n",
      "Epoch 902/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 424022.0938 - val_loss: 454995.0000\n",
      "Epoch 903/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415144.7188 - val_loss: 417944.3750\n",
      "Epoch 904/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424871.8750 - val_loss: 443196.4375\n",
      "Epoch 905/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 437288.1875 - val_loss: 411799.3438\n",
      "Epoch 906/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418542.3438 - val_loss: 419315.5312\n",
      "Epoch 907/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427168.4375 - val_loss: 410999.5938\n",
      "Epoch 908/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416626.2188 - val_loss: 435917.1562\n",
      "Epoch 909/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417369.2500 - val_loss: 439458.3125\n",
      "Epoch 910/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422637.3125 - val_loss: 415591.7188\n",
      "Epoch 911/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417074.7188 - val_loss: 412740.0625\n",
      "Epoch 912/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425007.9688 - val_loss: 483803.4062\n",
      "Epoch 913/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418064.1875 - val_loss: 485190.4375\n",
      "Epoch 914/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422470.3750 - val_loss: 440381.4062\n",
      "Epoch 915/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 433732.0312 - val_loss: 432464.1250\n",
      "Epoch 916/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416353.5312 - val_loss: 421941.6250\n",
      "Epoch 917/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410737.1875 - val_loss: 411380.9375\n",
      "Epoch 918/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425880.2188 - val_loss: 434739.9062\n",
      "Epoch 919/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413379.7500 - val_loss: 410311.6250\n",
      "Epoch 920/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423151.5312 - val_loss: 412450.1875\n",
      "Epoch 921/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425597.9688 - val_loss: 414234.1250\n",
      "Epoch 922/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413058.2500 - val_loss: 411897.0938\n",
      "Epoch 923/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421982.0938 - val_loss: 414824.7188\n",
      "Epoch 924/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422806.5625 - val_loss: 445472.4688\n",
      "Epoch 925/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421296.7500 - val_loss: 416653.6562\n",
      "Epoch 926/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416134.7188 - val_loss: 415947.2188\n",
      "Epoch 927/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418956.6875 - val_loss: 471427.5312\n",
      "Epoch 928/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 414322.2500 - val_loss: 411131.1562\n",
      "Epoch 929/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 415230.6562 - val_loss: 429871.4688\n",
      "Epoch 930/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422257.0312 - val_loss: 412108.2500\n",
      "Epoch 931/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 413751.8750 - val_loss: 411050.3750\n",
      "Epoch 932/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416060.0000 - val_loss: 411339.1250\n",
      "Epoch 933/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415805.2812 - val_loss: 410741.2812\n",
      "Epoch 934/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 421025.2188 - val_loss: 477668.3750\n",
      "Epoch 935/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423822.9375 - val_loss: 427577.0000\n",
      "Epoch 936/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 414228.2188 - val_loss: 414024.5000\n",
      "Epoch 937/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417626.7188 - val_loss: 421857.0938\n",
      "Epoch 938/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 422355.0938 - val_loss: 427634.3438\n",
      "Epoch 939/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429812.8125 - val_loss: 411368.0938\n",
      "Epoch 940/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 416827.0625 - val_loss: 455845.0312\n",
      "Epoch 941/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 413690.2500 - val_loss: 409520.6875\n",
      "Epoch 942/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410421.4375 - val_loss: 424037.4062\n",
      "Epoch 943/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410276.9688 - val_loss: 410994.6562\n",
      "Epoch 944/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421487.5625 - val_loss: 416766.6250\n",
      "Epoch 945/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414466.9375 - val_loss: 455377.6250\n",
      "Epoch 946/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432654.3438 - val_loss: 416469.3750\n",
      "Epoch 947/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414314.5625 - val_loss: 412276.9062\n",
      "Epoch 948/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410833.3125 - val_loss: 417696.6562\n",
      "Epoch 949/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425260.3438 - val_loss: 413314.0000\n",
      "Epoch 950/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 416037.5000 - val_loss: 499915.3438\n",
      "Epoch 951/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step - loss: 431823.0000 - val_loss: 426120.6250\n",
      "Epoch 952/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425678.8750 - val_loss: 424545.0625\n",
      "Epoch 953/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416890.1875 - val_loss: 423305.7188\n",
      "Epoch 954/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416618.7812 - val_loss: 412418.8750\n",
      "Epoch 955/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413580.4688 - val_loss: 410256.9375\n",
      "Epoch 956/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409089.4688 - val_loss: 414068.1875\n",
      "Epoch 957/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422555.0000 - val_loss: 429277.1875\n",
      "Epoch 958/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429282.0938 - val_loss: 414215.7500\n",
      "Epoch 959/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412358.3438 - val_loss: 425312.8438\n",
      "Epoch 960/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410995.5000 - val_loss: 411867.2812\n",
      "Epoch 961/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413893.7500 - val_loss: 414186.5312\n",
      "Epoch 962/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438027.0312 - val_loss: 414007.9062\n",
      "Epoch 963/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424009.8438 - val_loss: 442317.4062\n",
      "Epoch 964/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421048.5938 - val_loss: 411438.4062\n",
      "Epoch 965/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421534.1562 - val_loss: 418588.8750\n",
      "Epoch 966/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416268.5938 - val_loss: 412143.0938\n",
      "Epoch 967/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 429520.1875 - val_loss: 425627.1562\n",
      "Epoch 968/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419236.6562 - val_loss: 416266.0000\n",
      "Epoch 969/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411879.5625 - val_loss: 411185.0625\n",
      "Epoch 970/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417526.5625 - val_loss: 407215.1875\n",
      "Epoch 971/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412782.6250 - val_loss: 408139.1875\n",
      "Epoch 972/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419769.0000 - val_loss: 461253.0312\n",
      "Epoch 973/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411887.0625 - val_loss: 430761.8438\n",
      "Epoch 974/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418636.2500 - val_loss: 410001.9062\n",
      "Epoch 975/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421649.9688 - val_loss: 487130.1250\n",
      "Epoch 976/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418736.0000 - val_loss: 506433.8125\n",
      "Epoch 977/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413112.0000 - val_loss: 429156.5938\n",
      "Epoch 978/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428368.4375 - val_loss: 420358.8750\n",
      "Epoch 979/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418075.0625 - val_loss: 431792.8125\n",
      "Epoch 980/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419435.3438 - val_loss: 459670.8438\n",
      "Epoch 981/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424125.7188 - val_loss: 419893.1562\n",
      "Epoch 982/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411285.1875 - val_loss: 409862.9375\n",
      "Epoch 983/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420036.8750 - val_loss: 410056.1562\n",
      "Epoch 984/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417257.7812 - val_loss: 409326.2812\n",
      "Epoch 985/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411666.7812 - val_loss: 409693.2500\n",
      "Epoch 986/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416318.7500 - val_loss: 407307.6875\n",
      "Epoch 987/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 407832.3438 - val_loss: 420059.0312\n",
      "Epoch 988/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411689.1875 - val_loss: 418638.7812\n",
      "Epoch 989/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415191.4688 - val_loss: 478478.6562\n",
      "Epoch 990/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 446076.7500 - val_loss: 409585.8125\n",
      "Epoch 991/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424555.4688 - val_loss: 408378.4062\n",
      "Epoch 992/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420525.9375 - val_loss: 412513.6875\n",
      "Epoch 993/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419644.7188 - val_loss: 412796.2812\n",
      "Epoch 994/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410497.6562 - val_loss: 416604.8750\n",
      "Epoch 995/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416052.6250 - val_loss: 481878.2188\n",
      "Epoch 996/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415978.6875 - val_loss: 437213.8750\n",
      "Epoch 997/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416892.4062 - val_loss: 409027.3125\n",
      "Epoch 998/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426064.6562 - val_loss: 410023.7500\n",
      "Epoch 999/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411736.7812 - val_loss: 415266.9062\n",
      "Epoch 1000/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414345.6250 - val_loss: 413463.5938\n",
      "Epoch 1001/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 432389.9688 - val_loss: 412219.4062\n",
      "Epoch 1002/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417501.4062 - val_loss: 407728.7188\n",
      "Epoch 1003/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418407.4062 - val_loss: 418137.0625\n",
      "Epoch 1004/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424007.3750 - val_loss: 410731.0000\n",
      "Epoch 1005/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412118.0938 - val_loss: 446659.5625\n",
      "Epoch 1006/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418921.0000 - val_loss: 412345.2500\n",
      "Epoch 1007/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410554.1875 - val_loss: 419854.5938\n",
      "Epoch 1008/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 410530.3438 - val_loss: 407109.1875\n",
      "Epoch 1009/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 419620.6562 - val_loss: 407862.5312\n",
      "Epoch 1010/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409687.8125 - val_loss: 416164.2188\n",
      "Epoch 1011/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415539.5625 - val_loss: 412113.3750\n",
      "Epoch 1012/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410249.2500 - val_loss: 413945.8438\n",
      "Epoch 1013/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428674.4062 - val_loss: 495666.7812\n",
      "Epoch 1014/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418735.3438 - val_loss: 446508.6562\n",
      "Epoch 1015/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419995.2500 - val_loss: 410917.2500\n",
      "Epoch 1016/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411462.9688 - val_loss: 410257.7812\n",
      "Epoch 1017/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416757.2812 - val_loss: 412172.9062\n",
      "Epoch 1018/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409413.2188 - val_loss: 407905.0312\n",
      "Epoch 1019/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431412.1875 - val_loss: 431193.8750\n",
      "Epoch 1020/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425733.8125 - val_loss: 410649.8125\n",
      "Epoch 1021/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417629.7500 - val_loss: 416170.3750\n",
      "Epoch 1022/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417911.3125 - val_loss: 499989.9375\n",
      "Epoch 1023/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423683.2500 - val_loss: 412554.4375\n",
      "Epoch 1024/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step - loss: 415332.8438 - val_loss: 406743.6875\n",
      "Epoch 1025/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410810.9062 - val_loss: 411296.5625\n",
      "Epoch 1026/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424327.4375 - val_loss: 411258.5625\n",
      "Epoch 1027/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414152.2500 - val_loss: 412036.9062\n",
      "Epoch 1028/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409645.7500 - val_loss: 412237.0938\n",
      "Epoch 1029/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411128.7500 - val_loss: 415559.0625\n",
      "Epoch 1030/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415785.0625 - val_loss: 426718.1250\n",
      "Epoch 1031/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422655.5625 - val_loss: 424266.3750\n",
      "Epoch 1032/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 417827.3750 - val_loss: 425687.2812\n",
      "Epoch 1033/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 430033.0000 - val_loss: 432705.8750\n",
      "Epoch 1034/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410254.7812 - val_loss: 445086.0625\n",
      "Epoch 1035/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412696.3125 - val_loss: 412428.2500\n",
      "Epoch 1036/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419333.3125 - val_loss: 429212.4688\n",
      "Epoch 1037/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409278.0625 - val_loss: 414363.0000\n",
      "Epoch 1038/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408694.0625 - val_loss: 415115.2188\n",
      "Epoch 1039/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413992.3438 - val_loss: 447915.6250\n",
      "Epoch 1040/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424428.7500 - val_loss: 432092.1250\n",
      "Epoch 1041/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421284.7812 - val_loss: 411437.8750\n",
      "Epoch 1042/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423557.9688 - val_loss: 409775.7188\n",
      "Epoch 1043/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 407746.8750 - val_loss: 413042.7188\n",
      "Epoch 1044/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420083.7500 - val_loss: 410171.3125\n",
      "Epoch 1045/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408322.7500 - val_loss: 411892.3125\n",
      "Epoch 1046/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423603.0312 - val_loss: 407013.4688\n",
      "Epoch 1047/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419237.9062 - val_loss: 407787.0625\n",
      "Epoch 1048/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412179.6875 - val_loss: 412904.9688\n",
      "Epoch 1049/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418170.2500 - val_loss: 461101.7500\n",
      "Epoch 1050/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412813.0625 - val_loss: 492045.9375\n",
      "Epoch 1051/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 425968.8125 - val_loss: 405477.2500\n",
      "Epoch 1052/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411839.1875 - val_loss: 444827.2812\n",
      "Epoch 1053/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413764.2188 - val_loss: 409418.8125\n",
      "Epoch 1054/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428201.0625 - val_loss: 419245.5938\n",
      "Epoch 1055/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417492.6250 - val_loss: 406461.1875\n",
      "Epoch 1056/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 415445.1250 - val_loss: 415319.9688\n",
      "Epoch 1057/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408450.8438 - val_loss: 408817.7188\n",
      "Epoch 1058/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 436139.4375 - val_loss: 567118.5625\n",
      "Epoch 1059/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440665.7812 - val_loss: 432432.3125\n",
      "Epoch 1060/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411281.4688 - val_loss: 412260.9062\n",
      "Epoch 1061/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415843.2500 - val_loss: 421571.0938\n",
      "Epoch 1062/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415748.6875 - val_loss: 409429.5625\n",
      "Epoch 1063/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 405507.3125 - val_loss: 412307.5000\n",
      "Epoch 1064/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419180.5938 - val_loss: 414240.2188\n",
      "Epoch 1065/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412138.8750 - val_loss: 412449.0938\n",
      "Epoch 1066/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413404.9688 - val_loss: 406968.0625\n",
      "Epoch 1067/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411699.0625 - val_loss: 409318.2188\n",
      "Epoch 1068/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409227.7188 - val_loss: 434522.7188\n",
      "Epoch 1069/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416769.3125 - val_loss: 426242.2188\n",
      "Epoch 1070/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420701.9688 - val_loss: 491745.4375\n",
      "Epoch 1071/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424678.6562 - val_loss: 409006.9688\n",
      "Epoch 1072/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417184.0000 - val_loss: 415332.3750\n",
      "Epoch 1073/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411470.9688 - val_loss: 409233.0312\n",
      "Epoch 1074/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415686.9375 - val_loss: 412227.9688\n",
      "Epoch 1075/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424024.3750 - val_loss: 450007.6875\n",
      "Epoch 1076/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411451.8750 - val_loss: 408889.1250\n",
      "Epoch 1077/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416357.1875 - val_loss: 433664.8438\n",
      "Epoch 1078/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410704.3750 - val_loss: 407055.4375\n",
      "Epoch 1079/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426384.1562 - val_loss: 427617.4375\n",
      "Epoch 1080/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409220.5000 - val_loss: 409105.7812\n",
      "Epoch 1081/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417053.6250 - val_loss: 428986.6562\n",
      "Epoch 1082/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417623.9688 - val_loss: 419423.1562\n",
      "Epoch 1083/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412609.2188 - val_loss: 487455.5938\n",
      "Epoch 1084/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416768.2500 - val_loss: 418221.7500\n",
      "Epoch 1085/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416472.2812 - val_loss: 407991.1562\n",
      "Epoch 1086/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408261.8750 - val_loss: 412404.6875\n",
      "Epoch 1087/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412093.7812 - val_loss: 437264.5625\n",
      "Epoch 1088/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416310.9062 - val_loss: 408332.3750\n",
      "Epoch 1089/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411312.0938 - val_loss: 409594.4375\n",
      "Epoch 1090/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416849.8750 - val_loss: 408778.3438\n",
      "Epoch 1091/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409196.2500 - val_loss: 408924.3750\n",
      "Epoch 1092/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421068.0312 - val_loss: 465536.9688\n",
      "Epoch 1093/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420262.7188 - val_loss: 458802.4688\n",
      "Epoch 1094/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424828.2188 - val_loss: 406405.5625\n",
      "Epoch 1095/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422859.0938 - val_loss: 410040.1250\n",
      "Epoch 1096/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410170.5625 - val_loss: 413447.1562\n",
      "Epoch 1097/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409056.4062 - val_loss: 407977.7812\n",
      "Epoch 1098/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427212.5625 - val_loss: 414541.5625\n",
      "Epoch 1099/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414131.9062 - val_loss: 478373.6250\n",
      "Epoch 1100/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414670.9062 - val_loss: 425579.4375\n",
      "Epoch 1101/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417314.5000 - val_loss: 414476.1875\n",
      "Epoch 1102/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419195.7188 - val_loss: 421415.9688\n",
      "Epoch 1103/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408951.9375 - val_loss: 408634.9062\n",
      "Epoch 1104/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414164.6250 - val_loss: 419697.4688\n",
      "Epoch 1105/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 405164.0938 - val_loss: 405200.0625\n",
      "Epoch 1106/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421811.5312 - val_loss: 408490.8750\n",
      "Epoch 1107/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413998.8750 - val_loss: 409186.6250\n",
      "Epoch 1108/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409113.9062 - val_loss: 431095.4688\n",
      "Epoch 1109/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421992.8750 - val_loss: 410184.4688\n",
      "Epoch 1110/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410618.8125 - val_loss: 411326.4062\n",
      "Epoch 1111/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408084.9688 - val_loss: 426802.9688\n",
      "Epoch 1112/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408128.8125 - val_loss: 406661.9375\n",
      "Epoch 1113/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415716.4062 - val_loss: 422626.9375\n",
      "Epoch 1114/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412190.8438 - val_loss: 510059.6250\n",
      "Epoch 1115/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 426548.6562 - val_loss: 478713.7500\n",
      "Epoch 1116/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413527.8125 - val_loss: 420573.3125\n",
      "Epoch 1117/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410581.3438 - val_loss: 411462.1250\n",
      "Epoch 1118/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414344.6875 - val_loss: 406402.8438\n",
      "Epoch 1119/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414822.1250 - val_loss: 419897.0938\n",
      "Epoch 1120/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427228.6875 - val_loss: 415698.2812\n",
      "Epoch 1121/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414779.7500 - val_loss: 446956.6250\n",
      "Epoch 1122/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 406384.0312 - val_loss: 410565.5625\n",
      "Epoch 1123/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409802.4688 - val_loss: 414973.9375\n",
      "Epoch 1124/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421258.8438 - val_loss: 406111.4062\n",
      "Epoch 1125/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421672.5000 - val_loss: 410393.2812\n",
      "Epoch 1126/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412172.2188 - val_loss: 408344.9375\n",
      "Epoch 1127/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415217.9375 - val_loss: 409558.0312\n",
      "Epoch 1128/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412074.7188 - val_loss: 431461.4688\n",
      "Epoch 1129/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412812.5000 - val_loss: 423471.9375\n",
      "Epoch 1130/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410576.3125 - val_loss: 406224.4062\n",
      "Epoch 1131/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424368.3125 - val_loss: 405630.4062\n",
      "Epoch 1132/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409335.4062 - val_loss: 461350.9375\n",
      "Epoch 1133/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420178.5000 - val_loss: 406716.2188\n",
      "Epoch 1134/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408138.6250 - val_loss: 417071.4375\n",
      "Epoch 1135/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 406992.0625 - val_loss: 407245.5625\n",
      "Epoch 1136/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 405545.6875 - val_loss: 409165.5312\n",
      "Epoch 1137/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416230.5625 - val_loss: 427544.6562\n",
      "Epoch 1138/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 418667.4062 - val_loss: 405144.5000\n",
      "Epoch 1139/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 421722.4062 - val_loss: 414884.5312\n",
      "Epoch 1140/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 416793.2812 - val_loss: 410013.5938\n",
      "Epoch 1141/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 414668.5000 - val_loss: 406395.0625\n",
      "Epoch 1142/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 413393.4688 - val_loss: 409822.6875\n",
      "Epoch 1143/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411327.3125 - val_loss: 432544.7812\n",
      "Epoch 1144/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 428514.1562 - val_loss: 446037.6250\n",
      "Epoch 1145/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 410608.9375 - val_loss: 411619.3125\n",
      "Epoch 1146/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 413859.9375 - val_loss: 415193.9375\n",
      "Epoch 1147/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 415934.8750 - val_loss: 406938.6562\n",
      "Epoch 1148/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 411809.6562 - val_loss: 421293.7500\n",
      "Epoch 1149/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 416263.5000 - val_loss: 415495.7500\n",
      "Epoch 1150/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413269.2188 - val_loss: 409215.8438\n",
      "Epoch 1151/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 414992.9688 - val_loss: 410483.0312\n",
      "Epoch 1152/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 419527.3438 - val_loss: 458277.5312\n",
      "Epoch 1153/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 410109.2812 - val_loss: 454821.9375\n",
      "Epoch 1154/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416029.5625 - val_loss: 476687.0625\n",
      "Epoch 1155/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 428312.8125 - val_loss: 409479.9062\n",
      "Epoch 1156/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417274.5312 - val_loss: 407683.6562\n",
      "Epoch 1157/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 414335.8438 - val_loss: 503967.2188\n",
      "Epoch 1158/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 411553.0938 - val_loss: 404531.6875\n",
      "Epoch 1159/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 409215.5000 - val_loss: 407241.7188\n",
      "Epoch 1160/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409994.1562 - val_loss: 441336.3750\n",
      "Epoch 1161/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413354.0938 - val_loss: 407885.9375\n",
      "Epoch 1162/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411001.5938 - val_loss: 404638.6875\n",
      "Epoch 1163/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 418669.6875 - val_loss: 413047.0312\n",
      "Epoch 1164/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408203.3125 - val_loss: 425033.3438\n",
      "Epoch 1165/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408204.2812 - val_loss: 423473.0000\n",
      "Epoch 1166/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 431278.3125 - val_loss: 413101.5000\n",
      "Epoch 1167/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 422082.1875 - val_loss: 418444.3125\n",
      "Epoch 1168/1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 3ms/step - loss: 411000.0000 - val_loss: 413024.4688\n",
      "Epoch 1169/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 420911.7500 - val_loss: 422787.5938\n",
      "Epoch 1170/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410160.3438 - val_loss: 406157.8438\n",
      "Epoch 1171/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 416843.9688 - val_loss: 407010.4375\n",
      "Epoch 1172/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 409994.5312 - val_loss: 418735.7500\n",
      "Epoch 1173/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419918.4375 - val_loss: 405186.8438\n",
      "Epoch 1174/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 411609.6875 - val_loss: 494363.6562\n",
      "Epoch 1175/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 411033.8125 - val_loss: 405798.4375\n",
      "Epoch 1176/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422886.5000 - val_loss: 441600.1562\n",
      "Epoch 1177/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 409925.3750 - val_loss: 424852.8438\n",
      "Epoch 1178/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408402.6875 - val_loss: 409242.1562\n",
      "Epoch 1179/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413200.5000 - val_loss: 441131.7500\n",
      "Epoch 1180/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 420156.0625 - val_loss: 432062.0312\n",
      "Epoch 1181/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421436.1250 - val_loss: 407715.7812\n",
      "Epoch 1182/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423656.8750 - val_loss: 405222.4688\n",
      "Epoch 1183/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409136.2500 - val_loss: 407286.6250\n",
      "Epoch 1184/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410096.4688 - val_loss: 428624.1875\n",
      "Epoch 1185/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 407172.8750 - val_loss: 407417.9375\n",
      "Epoch 1186/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 443406.6875 - val_loss: 434369.7188\n",
      "Epoch 1187/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 407836.2812 - val_loss: 414465.0312\n",
      "Epoch 1188/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411979.9688 - val_loss: 419450.6875\n",
      "Epoch 1189/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 424322.5312 - val_loss: 409099.6250\n",
      "Epoch 1190/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 404943.3438 - val_loss: 431202.6875\n",
      "Epoch 1191/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411164.6250 - val_loss: 410773.5000\n",
      "Epoch 1192/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 427852.2188 - val_loss: 423106.0625\n",
      "Epoch 1193/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 406535.5000 - val_loss: 429507.9062\n",
      "Epoch 1194/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409158.4375 - val_loss: 420355.3438\n",
      "Epoch 1195/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412174.7812 - val_loss: 432890.9062\n",
      "Epoch 1196/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 417469.5312 - val_loss: 407357.8125\n",
      "Epoch 1197/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413298.8125 - val_loss: 421888.8750\n",
      "Epoch 1198/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411583.6250 - val_loss: 428757.6250\n",
      "Epoch 1199/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 418297.4688 - val_loss: 407661.3438\n",
      "Epoch 1200/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 429071.1562 - val_loss: 433448.8125\n",
      "Epoch 1201/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409034.3750 - val_loss: 409231.0312\n",
      "Epoch 1202/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411864.0938 - val_loss: 404862.3750\n",
      "Epoch 1203/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411884.0938 - val_loss: 424458.5938\n",
      "Epoch 1204/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409234.5938 - val_loss: 411907.9375\n",
      "Epoch 1205/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416251.3125 - val_loss: 424272.8125\n",
      "Epoch 1206/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414059.1562 - val_loss: 408113.1250\n",
      "Epoch 1207/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410243.6875 - val_loss: 412421.5625\n",
      "Epoch 1208/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410477.4375 - val_loss: 404610.5938\n",
      "Epoch 1209/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 419187.0938 - val_loss: 406257.7188\n",
      "Epoch 1210/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410943.1875 - val_loss: 405688.3438\n",
      "Epoch 1211/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413537.0938 - val_loss: 539795.8750\n",
      "Epoch 1212/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421999.2188 - val_loss: 406757.2500\n",
      "Epoch 1213/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 438989.7188 - val_loss: 484004.5312\n",
      "Epoch 1214/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 422074.0312 - val_loss: 432154.6562\n",
      "Epoch 1215/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 421077.0000 - val_loss: 415016.0312\n",
      "Epoch 1216/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 407935.2812 - val_loss: 404804.0938\n",
      "Epoch 1217/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416089.7188 - val_loss: 469821.1250\n",
      "Epoch 1218/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411997.5938 - val_loss: 482316.9375\n",
      "Epoch 1219/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410869.2188 - val_loss: 416703.0000\n",
      "Epoch 1220/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409913.0625 - val_loss: 408613.7500\n",
      "Epoch 1221/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 406410.6562 - val_loss: 407015.7188\n",
      "Epoch 1222/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 407623.1250 - val_loss: 412743.9375\n",
      "Epoch 1223/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412283.7812 - val_loss: 429881.3750\n",
      "Epoch 1224/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 409075.6875 - val_loss: 470033.2188\n",
      "Epoch 1225/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 435280.2500 - val_loss: 416352.7812\n",
      "Epoch 1226/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411160.1562 - val_loss: 426058.3750\n",
      "Epoch 1227/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 417882.9062 - val_loss: 404116.0938\n",
      "Epoch 1228/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416203.0938 - val_loss: 407518.1875\n",
      "Epoch 1229/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 405972.0000 - val_loss: 435969.7500\n",
      "Epoch 1230/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415818.0625 - val_loss: 419759.0000\n",
      "Epoch 1231/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412916.4688 - val_loss: 442907.9688\n",
      "Epoch 1232/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415208.9375 - val_loss: 426359.6562\n",
      "Epoch 1233/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 404560.2188 - val_loss: 403843.4375\n",
      "Epoch 1234/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408715.4062 - val_loss: 432487.7188\n",
      "Epoch 1235/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 406161.8125 - val_loss: 416457.4062\n",
      "Epoch 1236/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414133.3750 - val_loss: 406823.0000\n",
      "Epoch 1237/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411186.9375 - val_loss: 460878.9062\n",
      "Epoch 1238/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415599.2500 - val_loss: 405016.7812\n",
      "Epoch 1239/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 406123.8750 - val_loss: 417476.1875\n",
      "Epoch 1240/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408510.4375 - val_loss: 403595.4688\n",
      "Epoch 1241/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412132.8750 - val_loss: 441536.0000\n",
      "Epoch 1242/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416313.7188 - val_loss: 418360.6875\n",
      "Epoch 1243/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 440051.3438 - val_loss: 424582.6562\n",
      "Epoch 1244/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408439.0625 - val_loss: 405437.8750\n",
      "Epoch 1245/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415911.5000 - val_loss: 410860.2812\n",
      "Epoch 1246/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413425.2500 - val_loss: 409920.5000\n",
      "Epoch 1247/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414103.5938 - val_loss: 412257.6562\n",
      "Epoch 1248/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 416675.5625 - val_loss: 453271.2188\n",
      "Epoch 1249/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 405552.3125 - val_loss: 404724.8125\n",
      "Epoch 1250/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411614.0000 - val_loss: 453718.8125\n",
      "Epoch 1251/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 423732.0000 - val_loss: 465457.0938\n",
      "Epoch 1252/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 415306.4375 - val_loss: 444774.3438\n",
      "Epoch 1253/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 411262.0000 - val_loss: 411830.5000\n",
      "Epoch 1254/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 404876.8750 - val_loss: 433282.3750\n",
      "Epoch 1255/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 408317.7188 - val_loss: 403632.6250\n",
      "Epoch 1256/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 410047.1875 - val_loss: 415667.9062\n",
      "Epoch 1257/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 407128.3125 - val_loss: 419509.0625\n",
      "Epoch 1258/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 419025.2188 - val_loss: 513128.0938\n",
      "Epoch 1259/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 432639.7188 - val_loss: 418291.2188\n",
      "Epoch 1260/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 409543.1875 - val_loss: 404486.1250\n",
      "Epoch 1261/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 417650.1250 - val_loss: 403650.0000\n",
      "Epoch 1262/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 407043.1875 - val_loss: 404131.8750\n",
      "Epoch 1263/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 406449.1875 - val_loss: 432727.4062\n",
      "Epoch 1264/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 413810.1875 - val_loss: 407085.1250\n",
      "Epoch 1265/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 411917.7812 - val_loss: 413233.8125\n",
      "Epoch 1266/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 412275.9062 - val_loss: 410337.4375\n",
      "Epoch 1267/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 414645.5000 - val_loss: 405799.4375\n",
      "Epoch 1268/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 410079.0938 - val_loss: 405103.6250\n",
      "Epoch 1269/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 406261.4375 - val_loss: 422109.0938\n",
      "Epoch 1270/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 423950.5625 - val_loss: 411475.8750\n",
      "Epoch 1271/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 407675.2500 - val_loss: 431984.5000\n",
      "Epoch 1272/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 421440.0625 - val_loss: 403858.7812\n",
      "Epoch 1273/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 407909.1250 - val_loss: 419862.3125\n",
      "Epoch 1274/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 408642.9688 - val_loss: 417795.4375\n",
      "Epoch 1275/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 408909.3438 - val_loss: 405054.0312\n",
      "Epoch 1276/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 408107.2812 - val_loss: 452884.1875\n",
      "Epoch 1277/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 410354.0000 - val_loss: 412573.0938\n",
      "Epoch 1278/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 407061.5938 - val_loss: 406505.6562\n",
      "Epoch 1279/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 406551.3438 - val_loss: 405434.4688\n",
      "Epoch 1280/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 403781.4688 - val_loss: 413469.8438\n",
      "Epoch 1281/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 427500.0625 - val_loss: 471460.7812\n",
      "Epoch 1282/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 406373.3125 - val_loss: 404926.4375\n",
      "Epoch 1283/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 402799.3750 - val_loss: 416723.8750\n",
      "Epoch 1284/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 406330.0938 - val_loss: 405126.5625\n",
      "Epoch 1285/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 412053.3750 - val_loss: 432818.7188\n",
      "Epoch 1286/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 408226.4062 - val_loss: 404751.6562\n",
      "Epoch 1287/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 411957.4688 - val_loss: 409043.5000\n",
      "Epoch 1288/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 410291.3750 - val_loss: 425810.4062\n",
      "Epoch 1289/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 410142.0938 - val_loss: 416889.1250\n",
      "Epoch 1290/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 405212.3438 - val_loss: 404482.1875\n",
      "Epoch 1291/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 408816.2500 - val_loss: 459890.2812\n",
      "Epoch 1292/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 420200.2188 - val_loss: 466715.9688\n",
      "Epoch 1293/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 413039.2188 - val_loss: 415847.7188\n",
      "Epoch 1294/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 406207.7188 - val_loss: 408865.8125\n",
      "Epoch 1295/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 419229.4688 - val_loss: 427231.9375\n",
      "Epoch 1296/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 413902.9688 - val_loss: 416146.4688\n",
      "Epoch 1297/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 413847.7500 - val_loss: 427560.1562\n",
      "Epoch 1298/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 405979.0938 - val_loss: 404394.2812\n",
      "Epoch 1299/1300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 407388.8125 - val_loss: 407552.3750\n",
      "Epoch 1300/1300\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 406368.9688 - val_loss: 413598.5625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f501dc68d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=128,epochs=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 19)                342       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 19)                380       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 19)                380       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 19)                380       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 19)                380       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1882 (7.35 KB)\n",
      "Trainable params: 1882 (7.35 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df=pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "tahmin=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647248568194907"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,tahmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 5624004,
     "sourceId": 51959,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
